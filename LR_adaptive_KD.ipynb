{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-22T01:48:15.607Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/data/yaliu/jupyterbooks/multi-KD/models/teacher/resnet20.py:36: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(m.weight)\n",
      "/home/data/yaliu/jupyterbooks/multi-KD/models/student/resnet_s.py:13: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(m.weight)\n",
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "StudentNet:\n",
      "\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): LambdaLayer()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): LambdaLayer()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n",
      "Training adapter:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaliu/Dev/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:237: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/yaliu/Dev/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:176: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/yaliu/Dev/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/_reduction.py:15: UserWarning: reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(\"reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\")\n",
      "/home/yaliu/Dev/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:147: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/yaliu/Dev/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/functional.py:1992: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[0/70]adapter Loss: 1.9912\n",
      "epoch[1/70]adapter Loss: 1.7895\n",
      "epoch[2/70]adapter Loss: 1.4977\n",
      "epoch[3/70]adapter Loss: 1.3384\n",
      "epoch[4/70]adapter Loss: 1.2228\n",
      "epoch[5/70]adapter Loss: 1.3414\n",
      "epoch[6/70]adapter Loss: 1.2330\n",
      "epoch[7/70]adapter Loss: 0.8899\n",
      "epoch[8/70]adapter Loss: 1.0103\n",
      "epoch[9/70]adapter Loss: 0.8968\n",
      "epoch[10/70]adapter Loss: 0.8606\n",
      "epoch[11/70]adapter Loss: 1.1236\n",
      "epoch[12/70]adapter Loss: 0.8837\n",
      "epoch[13/70]adapter Loss: 0.7829\n",
      "epoch[14/70]adapter Loss: 0.8977\n",
      "epoch[15/70]adapter Loss: 0.6793\n",
      "epoch[16/70]adapter Loss: 0.8539\n",
      "epoch[17/70]adapter Loss: 0.9417\n",
      "epoch[18/70]adapter Loss: 0.8477\n",
      "epoch[19/70]adapter Loss: 0.7535\n",
      "epoch[20/70]adapter Loss: 0.7869\n",
      "epoch[21/70]adapter Loss: 0.9029\n",
      "epoch[22/70]adapter Loss: 0.7928\n",
      "epoch[23/70]adapter Loss: 0.6864\n",
      "epoch[24/70]adapter Loss: 0.9132\n",
      "epoch[25/70]adapter Loss: 0.9764\n",
      "epoch[26/70]adapter Loss: 0.8576\n",
      "epoch[27/70]adapter Loss: 0.9804\n",
      "epoch[28/70]adapter Loss: 0.6362\n",
      "epoch[29/70]adapter Loss: 0.8282\n",
      "epoch[30/70]adapter Loss: 0.9626\n",
      "epoch[31/70]adapter Loss: 0.9819\n",
      "epoch[32/70]adapter Loss: 0.9541\n",
      "epoch[33/70]adapter Loss: 0.8031\n",
      "epoch[34/70]adapter Loss: 0.6793\n",
      "epoch[35/70]adapter Loss: 0.8391\n",
      "epoch[36/70]adapter Loss: 0.8146\n",
      "epoch[37/70]adapter Loss: 0.8634\n",
      "epoch[38/70]adapter Loss: 0.7060\n",
      "epoch[39/70]adapter Loss: 0.9202\n",
      "epoch[40/70]adapter Loss: 0.7727\n",
      "epoch[41/70]adapter Loss: 0.7710\n",
      "epoch[42/70]adapter Loss: 0.6807\n",
      "epoch[43/70]adapter Loss: 0.9111\n",
      "epoch[44/70]adapter Loss: 0.7290\n",
      "epoch[45/70]adapter Loss: 0.9251\n",
      "epoch[46/70]adapter Loss: 0.9201\n",
      "epoch[47/70]adapter Loss: 0.8843\n",
      "epoch[48/70]adapter Loss: 0.5732\n",
      "epoch[49/70]adapter Loss: 0.9679\n",
      "epoch[50/70]adapter Loss: 0.6171\n",
      "epoch[51/70]adapter Loss: 0.9027\n",
      "epoch[52/70]adapter Loss: 0.9872\n",
      "epoch[53/70]adapter Loss: 0.6111\n",
      "epoch[54/70]adapter Loss: 0.7847\n",
      "epoch[55/70]adapter Loss: 0.8880\n",
      "epoch[56/70]adapter Loss: 0.6865\n",
      "epoch[57/70]adapter Loss: 0.7396\n",
      "epoch[58/70]adapter Loss: 0.8754\n",
      "epoch[59/70]adapter Loss: 0.7967\n",
      "epoch[60/70]adapter Loss: 0.6824\n",
      "epoch[61/70]adapter Loss: 0.9893\n",
      "epoch[62/70]adapter Loss: 0.7754\n",
      "epoch[63/70]adapter Loss: 0.5475\n",
      "epoch[64/70]adapter Loss: 0.8002\n",
      "epoch[65/70]adapter Loss: 0.9395\n",
      "epoch[66/70]adapter Loss: 0.7613\n",
      "epoch[67/70]adapter Loss: 0.7552\n",
      "epoch[68/70]adapter Loss: 0.6690\n",
      "epoch[69/70]adapter Loss: 0.6835\n",
      "--- adapter training cost 28.781 mins ---\n",
      "\n",
      "===> epoch: 1/200\n",
      "current lr 1.00000e-01\n",
      "Training:\n",
      "[0/391]\tTime 0.068 (0.068)\tData 0.021 (0.021)\tLoss 0.7339 (0.7339)\tPrec@1 80.469 (80.469)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaliu/Dev/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:291: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/391]\tTime 0.066 (0.067)\tData 0.019 (0.019)\tLoss 0.8254 (0.7442)\tPrec@1 78.906 (81.179)\n",
      "[20/391]\tTime 0.068 (0.067)\tData 0.019 (0.019)\tLoss 0.7429 (0.7140)\tPrec@1 78.125 (81.845)\n",
      "[30/391]\tTime 0.069 (0.067)\tData 0.019 (0.019)\tLoss 0.7722 (0.7420)\tPrec@1 77.344 (81.048)\n",
      "[40/391]\tTime 0.067 (0.068)\tData 0.018 (0.019)\tLoss 0.8041 (0.7452)\tPrec@1 76.562 (81.136)\n",
      "[50/391]\tTime 0.072 (0.068)\tData 0.019 (0.019)\tLoss 0.6607 (0.7487)\tPrec@1 81.250 (80.974)\n",
      "[60/391]\tTime 0.066 (0.068)\tData 0.019 (0.019)\tLoss 0.6713 (0.7427)\tPrec@1 84.375 (81.301)\n",
      "[70/391]\tTime 0.066 (0.068)\tData 0.019 (0.019)\tLoss 0.5587 (0.7462)\tPrec@1 86.719 (81.173)\n",
      "[80/391]\tTime 0.067 (0.068)\tData 0.019 (0.019)\tLoss 0.7752 (0.7487)\tPrec@1 78.906 (81.096)\n",
      "[90/391]\tTime 0.071 (0.068)\tData 0.018 (0.019)\tLoss 0.7360 (0.7443)\tPrec@1 82.812 (81.216)\n",
      "[100/391]\tTime 0.066 (0.068)\tData 0.018 (0.019)\tLoss 0.6633 (0.7422)\tPrec@1 82.812 (81.250)\n",
      "[110/391]\tTime 0.066 (0.068)\tData 0.019 (0.019)\tLoss 0.6944 (0.7402)\tPrec@1 82.812 (81.285)\n",
      "[120/391]\tTime 0.066 (0.068)\tData 0.019 (0.019)\tLoss 0.7595 (0.7464)\tPrec@1 84.375 (81.095)\n",
      "[130/391]\tTime 0.067 (0.068)\tData 0.019 (0.019)\tLoss 0.7685 (0.7463)\tPrec@1 80.469 (81.119)\n",
      "[140/391]\tTime 0.067 (0.068)\tData 0.020 (0.019)\tLoss 0.8743 (0.7480)\tPrec@1 79.688 (81.084)\n",
      "[150/391]\tTime 0.065 (0.068)\tData 0.018 (0.019)\tLoss 0.6125 (0.7480)\tPrec@1 85.938 (81.141)\n",
      "[160/391]\tTime 0.074 (0.068)\tData 0.019 (0.019)\tLoss 0.9269 (0.7517)\tPrec@1 76.562 (81.041)\n",
      "[170/391]\tTime 0.072 (0.068)\tData 0.019 (0.019)\tLoss 0.7814 (0.7518)\tPrec@1 82.031 (80.994)\n",
      "[180/391]\tTime 0.069 (0.068)\tData 0.019 (0.019)\tLoss 0.8448 (0.7505)\tPrec@1 78.906 (81.030)\n",
      "[190/391]\tTime 0.066 (0.068)\tData 0.018 (0.019)\tLoss 0.7165 (0.7495)\tPrec@1 84.375 (81.127)\n",
      "[200/391]\tTime 0.066 (0.068)\tData 0.019 (0.019)\tLoss 0.5599 (0.7482)\tPrec@1 87.500 (81.172)\n",
      "[210/391]\tTime 0.071 (0.068)\tData 0.019 (0.019)\tLoss 0.7858 (0.7490)\tPrec@1 81.250 (81.169)\n",
      "[220/391]\tTime 0.068 (0.068)\tData 0.018 (0.019)\tLoss 0.7560 (0.7486)\tPrec@1 80.469 (81.183)\n",
      "[230/391]\tTime 0.080 (0.068)\tData 0.031 (0.019)\tLoss 0.8023 (0.7477)\tPrec@1 78.906 (81.233)\n",
      "[240/391]\tTime 0.067 (0.068)\tData 0.019 (0.019)\tLoss 0.7293 (0.7467)\tPrec@1 80.469 (81.269)\n",
      "[250/391]\tTime 0.067 (0.068)\tData 0.018 (0.019)\tLoss 0.7310 (0.7464)\tPrec@1 78.906 (81.259)\n",
      "[260/391]\tTime 0.067 (0.068)\tData 0.018 (0.019)\tLoss 0.7158 (0.7489)\tPrec@1 82.812 (81.196)\n",
      "[270/391]\tTime 0.066 (0.068)\tData 0.019 (0.019)\tLoss 0.6324 (0.7482)\tPrec@1 84.375 (81.259)\n",
      "[280/391]\tTime 0.065 (0.068)\tData 0.018 (0.019)\tLoss 0.7306 (0.7489)\tPrec@1 82.812 (81.217)\n",
      "[290/391]\tTime 0.069 (0.068)\tData 0.021 (0.019)\tLoss 0.6653 (0.7488)\tPrec@1 82.031 (81.263)\n",
      "[300/391]\tTime 0.068 (0.068)\tData 0.019 (0.019)\tLoss 0.7646 (0.7470)\tPrec@1 78.125 (81.297)\n",
      "[310/391]\tTime 0.065 (0.068)\tData 0.018 (0.019)\tLoss 0.6322 (0.7487)\tPrec@1 84.375 (81.293)\n",
      "[320/391]\tTime 0.065 (0.068)\tData 0.018 (0.019)\tLoss 0.5754 (0.7491)\tPrec@1 88.281 (81.250)\n",
      "[330/391]\tTime 0.067 (0.068)\tData 0.019 (0.019)\tLoss 0.6800 (0.7501)\tPrec@1 81.250 (81.210)\n",
      "[340/391]\tTime 0.069 (0.068)\tData 0.018 (0.019)\tLoss 0.8430 (0.7510)\tPrec@1 80.469 (81.186)\n",
      "[350/391]\tTime 0.070 (0.068)\tData 0.018 (0.019)\tLoss 0.7704 (0.7507)\tPrec@1 79.688 (81.212)\n",
      "[360/391]\tTime 0.066 (0.068)\tData 0.019 (0.019)\tLoss 0.8504 (0.7506)\tPrec@1 75.781 (81.211)\n",
      "[370/391]\tTime 0.066 (0.068)\tData 0.018 (0.019)\tLoss 0.7895 (0.7516)\tPrec@1 79.688 (81.181)\n",
      "[380/391]\tTime 0.072 (0.068)\tData 0.019 (0.019)\tLoss 0.7499 (0.7516)\tPrec@1 81.250 (81.201)\n",
      "[390/391]\tTime 0.048 (0.068)\tData 0.012 (0.019)\tLoss 0.8992 (0.7528)\tPrec@1 75.000 (81.168)\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.018 (0.018)\tLoss 1.6802 (1.6802)\tPrec@1 61.719 (61.719)\n",
      "Test: [10/79]\tTime 0.017 (0.017)\tLoss 1.3600 (1.7012)\tPrec@1 72.656 (66.477)\n",
      "Test: [20/79]\tTime 0.018 (0.018)\tLoss 1.7013 (1.7016)\tPrec@1 64.844 (66.741)\n",
      "Test: [30/79]\tTime 0.017 (0.017)\tLoss 1.5130 (1.7012)\tPrec@1 73.438 (67.112)\n",
      "Test: [40/79]\tTime 0.018 (0.018)\tLoss 1.8357 (1.6819)\tPrec@1 61.719 (67.188)\n",
      "Test: [50/79]\tTime 0.017 (0.018)\tLoss 1.7594 (1.6839)\tPrec@1 62.500 (66.896)\n",
      "Test: [60/79]\tTime 0.018 (0.018)\tLoss 1.8912 (1.6840)\tPrec@1 64.844 (66.957)\n",
      "Test: [70/79]\tTime 0.017 (0.018)\tLoss 1.6654 (1.6797)\tPrec@1 70.312 (67.066)\n",
      " * Prec@1 67.150\n",
      "\n",
      "===> epoch: 2/200\n",
      "current lr 1.00000e-01\n",
      "Training:\n",
      "[0/391]\tTime 0.071 (0.071)\tData 0.020 (0.020)\tLoss 0.6414 (0.6414)\tPrec@1 82.812 (82.812)\n",
      "[10/391]\tTime 0.067 (0.070)\tData 0.018 (0.019)\tLoss 0.8465 (0.7531)\tPrec@1 78.125 (82.173)\n",
      "[20/391]\tTime 0.065 (0.068)\tData 0.018 (0.019)\tLoss 0.8783 (0.7752)\tPrec@1 81.250 (81.585)\n",
      "[30/391]\tTime 0.067 (0.068)\tData 0.019 (0.019)\tLoss 0.8699 (0.7746)\tPrec@1 79.688 (81.200)\n",
      "[40/391]\tTime 0.066 (0.067)\tData 0.018 (0.019)\tLoss 0.6676 (0.7734)\tPrec@1 87.500 (81.002)\n",
      "[50/391]\tTime 0.074 (0.067)\tData 0.024 (0.019)\tLoss 0.7445 (0.7825)\tPrec@1 80.469 (80.760)\n",
      "[60/391]\tTime 0.068 (0.068)\tData 0.018 (0.019)\tLoss 0.7347 (0.7798)\tPrec@1 83.594 (80.904)\n",
      "[70/391]\tTime 0.070 (0.068)\tData 0.018 (0.019)\tLoss 0.8193 (0.7743)\tPrec@1 78.906 (80.744)\n",
      "[80/391]\tTime 0.067 (0.068)\tData 0.019 (0.019)\tLoss 0.8704 (0.7750)\tPrec@1 74.219 (80.758)\n",
      "[90/391]\tTime 0.068 (0.068)\tData 0.020 (0.019)\tLoss 0.9501 (0.7746)\tPrec@1 76.562 (80.786)\n",
      "[100/391]\tTime 0.065 (0.068)\tData 0.018 (0.019)\tLoss 0.6666 (0.7687)\tPrec@1 82.812 (80.964)\n",
      "[110/391]\tTime 0.066 (0.067)\tData 0.019 (0.019)\tLoss 0.8478 (0.7630)\tPrec@1 75.781 (81.137)\n",
      "[120/391]\tTime 0.065 (0.068)\tData 0.018 (0.019)\tLoss 0.9007 (0.7574)\tPrec@1 76.562 (81.276)\n",
      "[130/391]\tTime 0.066 (0.067)\tData 0.018 (0.019)\tLoss 0.7522 (0.7561)\tPrec@1 80.469 (81.232)\n",
      "[140/391]\tTime 0.068 (0.068)\tData 0.018 (0.019)\tLoss 0.6226 (0.7535)\tPrec@1 86.719 (81.294)\n",
      "[150/391]\tTime 0.070 (0.068)\tData 0.019 (0.019)\tLoss 0.8699 (0.7537)\tPrec@1 76.562 (81.245)\n",
      "[160/391]\tTime 0.065 (0.068)\tData 0.018 (0.019)\tLoss 0.6307 (0.7525)\tPrec@1 83.594 (81.299)\n",
      "[170/391]\tTime 0.082 (0.068)\tData 0.033 (0.019)\tLoss 1.0082 (0.7533)\tPrec@1 70.312 (81.232)\n",
      "[180/391]\tTime 0.067 (0.068)\tData 0.018 (0.019)\tLoss 0.7579 (0.7534)\tPrec@1 82.031 (81.203)\n",
      "[190/391]\tTime 0.068 (0.068)\tData 0.018 (0.019)\tLoss 0.7369 (0.7526)\tPrec@1 81.250 (81.238)\n",
      "[200/391]\tTime 0.071 (0.068)\tData 0.019 (0.019)\tLoss 0.7107 (0.7542)\tPrec@1 81.250 (81.215)\n",
      "[210/391]\tTime 0.068 (0.068)\tData 0.018 (0.019)\tLoss 0.6105 (0.7514)\tPrec@1 85.156 (81.298)\n",
      "[220/391]\tTime 0.066 (0.068)\tData 0.018 (0.019)\tLoss 0.6538 (0.7520)\tPrec@1 82.812 (81.264)\n",
      "[230/391]\tTime 0.071 (0.068)\tData 0.018 (0.019)\tLoss 0.8590 (0.7497)\tPrec@1 76.562 (81.321)\n",
      "[240/391]\tTime 0.065 (0.068)\tData 0.018 (0.019)\tLoss 0.6571 (0.7483)\tPrec@1 85.156 (81.370)\n",
      "[250/391]\tTime 0.067 (0.068)\tData 0.018 (0.019)\tLoss 0.7897 (0.7484)\tPrec@1 79.688 (81.387)\n",
      "[260/391]\tTime 0.067 (0.068)\tData 0.018 (0.019)\tLoss 0.6483 (0.7480)\tPrec@1 82.812 (81.394)\n",
      "[270/391]\tTime 0.067 (0.068)\tData 0.018 (0.019)\tLoss 0.6365 (0.7483)\tPrec@1 85.938 (81.334)\n",
      "[280/391]\tTime 0.070 (0.068)\tData 0.022 (0.019)\tLoss 0.7248 (0.7497)\tPrec@1 77.344 (81.289)\n",
      "[290/391]\tTime 0.065 (0.068)\tData 0.018 (0.019)\tLoss 0.9265 (0.7485)\tPrec@1 79.688 (81.296)\n",
      "[300/391]\tTime 0.067 (0.068)\tData 0.018 (0.019)\tLoss 0.6811 (0.7494)\tPrec@1 87.500 (81.323)\n",
      "[310/391]\tTime 0.071 (0.068)\tData 0.018 (0.019)\tLoss 0.8541 (0.7494)\tPrec@1 76.562 (81.313)\n",
      "[320/391]\tTime 0.067 (0.068)\tData 0.018 (0.019)\tLoss 0.9359 (0.7501)\tPrec@1 76.562 (81.296)\n",
      "[330/391]\tTime 0.065 (0.068)\tData 0.018 (0.019)\tLoss 0.6596 (0.7496)\tPrec@1 86.719 (81.290)\n",
      "[340/391]\tTime 0.065 (0.068)\tData 0.018 (0.019)\tLoss 0.7372 (0.7506)\tPrec@1 80.469 (81.239)\n",
      "[350/391]\tTime 0.066 (0.068)\tData 0.019 (0.019)\tLoss 0.6855 (0.7503)\tPrec@1 75.781 (81.272)\n",
      "[360/391]\tTime 0.066 (0.068)\tData 0.019 (0.019)\tLoss 0.9584 (0.7514)\tPrec@1 77.344 (81.237)\n",
      "[370/391]\tTime 0.072 (0.068)\tData 0.022 (0.019)\tLoss 0.8066 (0.7517)\tPrec@1 79.688 (81.239)\n",
      "[380/391]\tTime 0.066 (0.068)\tData 0.018 (0.019)\tLoss 0.9320 (0.7538)\tPrec@1 80.469 (81.217)\n",
      "[390/391]\tTime 0.045 (0.067)\tData 0.012 (0.019)\tLoss 0.8315 (0.7544)\tPrec@1 80.000 (81.198)\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.018 (0.018)\tLoss 0.7692 (0.7692)\tPrec@1 82.031 (82.031)\n",
      "Test: [10/79]\tTime 0.017 (0.018)\tLoss 0.7534 (0.9081)\tPrec@1 83.594 (77.202)\n",
      "Test: [20/79]\tTime 0.018 (0.018)\tLoss 0.9654 (0.9492)\tPrec@1 76.562 (76.153)\n",
      "Test: [30/79]\tTime 0.017 (0.018)\tLoss 0.6460 (0.9455)\tPrec@1 82.812 (76.361)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [40/79]\tTime 0.017 (0.018)\tLoss 0.8105 (0.9277)\tPrec@1 81.250 (77.115)\n",
      "Test: [50/79]\tTime 0.017 (0.017)\tLoss 0.8751 (0.9163)\tPrec@1 77.344 (77.206)\n",
      "Test: [60/79]\tTime 0.017 (0.017)\tLoss 0.8806 (0.9182)\tPrec@1 78.906 (77.267)\n",
      "Test: [70/79]\tTime 0.017 (0.017)\tLoss 0.6500 (0.9187)\tPrec@1 81.250 (77.113)\n",
      " * Prec@1 77.170\n",
      "\n",
      "===> epoch: 3/200\n",
      "current lr 1.00000e-01\n",
      "Training:\n",
      "[0/391]\tTime 0.077 (0.077)\tData 0.023 (0.023)\tLoss 0.8637 (0.8637)\tPrec@1 78.125 (78.125)\n",
      "[10/391]\tTime 0.069 (0.070)\tData 0.019 (0.019)\tLoss 0.6988 (0.7661)\tPrec@1 78.906 (80.611)\n",
      "[20/391]\tTime 0.065 (0.068)\tData 0.018 (0.019)\tLoss 0.7433 (0.7608)\tPrec@1 81.250 (80.729)\n",
      "[30/391]\tTime 0.067 (0.068)\tData 0.019 (0.019)\tLoss 0.8730 (0.7586)\tPrec@1 78.906 (80.922)\n",
      "[40/391]\tTime 0.066 (0.067)\tData 0.019 (0.019)\tLoss 0.8082 (0.7353)\tPrec@1 80.469 (81.669)\n",
      "[50/391]\tTime 0.066 (0.067)\tData 0.019 (0.019)\tLoss 0.7477 (0.7365)\tPrec@1 83.594 (81.464)\n",
      "[60/391]\tTime 0.066 (0.067)\tData 0.018 (0.019)\tLoss 0.7732 (0.7512)\tPrec@1 78.906 (80.955)\n",
      "[70/391]\tTime 0.068 (0.067)\tData 0.019 (0.019)\tLoss 0.8501 (0.7622)\tPrec@1 78.125 (80.733)\n",
      "[80/391]\tTime 0.066 (0.067)\tData 0.018 (0.019)\tLoss 0.5974 (0.7634)\tPrec@1 87.500 (80.787)\n",
      "[90/391]\tTime 0.069 (0.068)\tData 0.019 (0.019)\tLoss 0.7056 (0.7619)\tPrec@1 84.375 (80.855)\n",
      "[100/391]\tTime 0.068 (0.068)\tData 0.018 (0.019)\tLoss 0.6768 (0.7660)\tPrec@1 80.469 (80.554)\n",
      "[110/391]\tTime 0.067 (0.068)\tData 0.019 (0.019)\tLoss 0.6540 (0.7616)\tPrec@1 83.594 (80.715)\n",
      "[120/391]\tTime 0.067 (0.068)\tData 0.019 (0.019)\tLoss 0.6548 (0.7589)\tPrec@1 83.594 (80.863)\n",
      "[130/391]\tTime 0.068 (0.068)\tData 0.019 (0.019)\tLoss 0.9272 (0.7607)\tPrec@1 78.125 (80.809)\n",
      "[140/391]\tTime 0.068 (0.068)\tData 0.019 (0.019)\tLoss 0.6377 (0.7605)\tPrec@1 83.594 (80.801)\n",
      "[150/391]\tTime 0.072 (0.068)\tData 0.019 (0.019)\tLoss 0.7805 (0.7597)\tPrec@1 79.688 (80.852)\n",
      "[160/391]\tTime 0.069 (0.068)\tData 0.019 (0.019)\tLoss 0.8650 (0.7577)\tPrec@1 81.250 (80.935)\n",
      "[170/391]\tTime 0.065 (0.068)\tData 0.018 (0.019)\tLoss 0.6552 (0.7556)\tPrec@1 83.594 (81.031)\n",
      "[180/391]\tTime 0.066 (0.068)\tData 0.019 (0.019)\tLoss 0.7965 (0.7558)\tPrec@1 80.469 (81.013)\n",
      "[190/391]\tTime 0.066 (0.068)\tData 0.018 (0.019)\tLoss 0.8633 (0.7565)\tPrec@1 78.906 (80.972)\n",
      "[200/391]\tTime 0.066 (0.068)\tData 0.019 (0.019)\tLoss 0.8488 (0.7564)\tPrec@1 78.125 (80.896)\n",
      "[210/391]\tTime 0.066 (0.068)\tData 0.018 (0.019)\tLoss 0.7735 (0.7576)\tPrec@1 83.594 (80.906)\n",
      "[220/391]\tTime 0.067 (0.068)\tData 0.019 (0.019)\tLoss 0.6282 (0.7574)\tPrec@1 86.719 (80.935)\n",
      "[230/391]\tTime 0.067 (0.068)\tData 0.019 (0.019)\tLoss 0.7696 (0.7583)\tPrec@1 82.031 (80.861)\n",
      "[240/391]\tTime 0.065 (0.067)\tData 0.018 (0.019)\tLoss 0.7076 (0.7589)\tPrec@1 82.812 (80.874)\n",
      "[250/391]\tTime 0.068 (0.067)\tData 0.019 (0.019)\tLoss 0.7842 (0.7576)\tPrec@1 84.375 (80.898)\n",
      "[260/391]\tTime 0.071 (0.067)\tData 0.018 (0.019)\tLoss 0.8832 (0.7588)\tPrec@1 75.781 (80.867)\n",
      "[270/391]\tTime 0.069 (0.068)\tData 0.018 (0.019)\tLoss 0.9198 (0.7601)\tPrec@1 77.344 (80.846)\n",
      "[280/391]\tTime 0.067 (0.068)\tData 0.019 (0.019)\tLoss 0.9060 (0.7609)\tPrec@1 78.125 (80.841)\n",
      "[290/391]\tTime 0.065 (0.068)\tData 0.018 (0.019)\tLoss 0.6937 (0.7608)\tPrec@1 86.719 (80.861)\n",
      "[300/391]\tTime 0.066 (0.068)\tData 0.019 (0.019)\tLoss 0.8551 (0.7600)\tPrec@1 75.781 (80.840)\n",
      "[310/391]\tTime 0.065 (0.067)\tData 0.018 (0.019)\tLoss 0.7500 (0.7593)\tPrec@1 80.469 (80.878)\n",
      "[320/391]\tTime 0.065 (0.067)\tData 0.018 (0.019)\tLoss 0.7508 (0.7595)\tPrec@1 78.125 (80.846)\n",
      "[330/391]\tTime 0.066 (0.067)\tData 0.019 (0.019)\tLoss 0.7070 (0.7586)\tPrec@1 83.594 (80.879)\n",
      "[340/391]\tTime 0.068 (0.067)\tData 0.019 (0.019)\tLoss 0.6890 (0.7585)\tPrec@1 79.688 (80.877)\n",
      "[350/391]\tTime 0.065 (0.067)\tData 0.018 (0.019)\tLoss 0.6830 (0.7570)\tPrec@1 82.812 (80.914)\n",
      "[360/391]\tTime 0.065 (0.067)\tData 0.018 (0.019)\tLoss 0.8023 (0.7557)\tPrec@1 78.906 (80.934)\n",
      "[370/391]\tTime 0.065 (0.067)\tData 0.018 (0.019)\tLoss 0.8985 (0.7558)\tPrec@1 78.125 (80.943)\n",
      "[380/391]\tTime 0.066 (0.067)\tData 0.019 (0.019)\tLoss 0.6836 (0.7564)\tPrec@1 85.938 (80.906)\n",
      "[390/391]\tTime 0.045 (0.067)\tData 0.012 (0.019)\tLoss 0.7989 (0.7576)\tPrec@1 78.750 (80.864)\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.017 (0.017)\tLoss 1.2069 (1.2069)\tPrec@1 72.656 (72.656)\n",
      "Test: [10/79]\tTime 0.017 (0.017)\tLoss 1.4035 (1.5284)\tPrec@1 68.750 (66.619)\n",
      "Test: [20/79]\tTime 0.018 (0.017)\tLoss 1.8755 (1.6156)\tPrec@1 64.062 (66.815)\n",
      "Test: [30/79]\tTime 0.017 (0.017)\tLoss 1.2760 (1.5633)\tPrec@1 68.750 (67.162)\n",
      "Test: [40/79]\tTime 0.017 (0.017)\tLoss 1.8627 (1.5420)\tPrec@1 64.844 (67.893)\n",
      "Test: [50/79]\tTime 0.017 (0.017)\tLoss 1.6878 (1.5360)\tPrec@1 66.406 (68.122)\n",
      "Test: [60/79]\tTime 0.017 (0.017)\tLoss 1.6315 (1.5232)\tPrec@1 71.875 (68.327)\n",
      "Test: [70/79]\tTime 0.018 (0.017)\tLoss 1.3436 (1.5388)\tPrec@1 69.531 (68.112)\n",
      " * Prec@1 68.110\n",
      "\n",
      "===> epoch: 4/200\n",
      "current lr 1.00000e-01\n",
      "Training:\n",
      "[0/391]\tTime 0.072 (0.072)\tData 0.020 (0.020)\tLoss 0.6375 (0.6375)\tPrec@1 83.594 (83.594)\n",
      "[10/391]\tTime 0.070 (0.071)\tData 0.018 (0.018)\tLoss 0.8126 (0.7466)\tPrec@1 79.688 (81.960)\n",
      "[20/391]\tTime 0.066 (0.069)\tData 0.019 (0.019)\tLoss 0.8620 (0.7612)\tPrec@1 78.906 (81.845)\n",
      "[30/391]\tTime 0.071 (0.069)\tData 0.019 (0.019)\tLoss 0.7667 (0.7541)\tPrec@1 83.594 (81.527)\n",
      "[40/391]\tTime 0.071 (0.069)\tData 0.019 (0.019)\tLoss 0.6680 (0.7405)\tPrec@1 85.156 (81.974)\n",
      "[50/391]\tTime 0.065 (0.069)\tData 0.018 (0.019)\tLoss 0.5379 (0.7390)\tPrec@1 87.500 (81.786)\n",
      "[60/391]\tTime 0.065 (0.068)\tData 0.018 (0.019)\tLoss 0.6433 (0.7320)\tPrec@1 81.250 (81.865)\n",
      "[70/391]\tTime 0.068 (0.068)\tData 0.019 (0.019)\tLoss 0.8737 (0.7341)\tPrec@1 74.219 (81.679)\n",
      "[80/391]\tTime 0.065 (0.068)\tData 0.018 (0.019)\tLoss 0.7725 (0.7336)\tPrec@1 78.906 (81.636)\n",
      "[90/391]\tTime 0.081 (0.068)\tData 0.033 (0.019)\tLoss 0.7368 (0.7353)\tPrec@1 80.469 (81.473)\n",
      "[100/391]\tTime 0.065 (0.068)\tData 0.018 (0.019)\tLoss 0.6830 (0.7361)\tPrec@1 81.250 (81.412)\n",
      "[110/391]\tTime 0.066 (0.068)\tData 0.019 (0.019)\tLoss 0.7718 (0.7358)\tPrec@1 78.125 (81.391)\n",
      "[120/391]\tTime 0.066 (0.067)\tData 0.019 (0.019)\tLoss 0.5612 (0.7347)\tPrec@1 89.062 (81.411)\n",
      "[130/391]\tTime 0.065 (0.067)\tData 0.018 (0.019)\tLoss 0.8146 (0.7329)\tPrec@1 82.031 (81.477)\n",
      "[140/391]\tTime 0.066 (0.067)\tData 0.018 (0.019)\tLoss 0.8196 (0.7373)\tPrec@1 84.375 (81.377)\n",
      "[150/391]\tTime 0.066 (0.067)\tData 0.018 (0.019)\tLoss 0.8743 (0.7403)\tPrec@1 79.688 (81.333)\n",
      "[160/391]\tTime 0.065 (0.067)\tData 0.018 (0.019)\tLoss 0.8094 (0.7422)\tPrec@1 78.125 (81.328)\n",
      "[170/391]\tTime 0.066 (0.067)\tData 0.018 (0.019)\tLoss 0.8283 (0.7476)\tPrec@1 82.031 (81.177)\n",
      "[180/391]\tTime 0.066 (0.067)\tData 0.019 (0.019)\tLoss 0.8013 (0.7482)\tPrec@1 81.250 (81.211)\n",
      "[190/391]\tTime 0.065 (0.067)\tData 0.018 (0.019)\tLoss 0.8132 (0.7498)\tPrec@1 80.469 (81.185)\n",
      "[200/391]\tTime 0.066 (0.067)\tData 0.019 (0.019)\tLoss 0.9227 (0.7512)\tPrec@1 77.344 (81.157)\n",
      "[210/391]\tTime 0.071 (0.067)\tData 0.018 (0.019)\tLoss 0.8651 (0.7505)\tPrec@1 75.000 (81.143)\n",
      "[220/391]\tTime 0.066 (0.067)\tData 0.019 (0.019)\tLoss 0.6311 (0.7510)\tPrec@1 87.500 (81.116)\n",
      "[230/391]\tTime 0.067 (0.067)\tData 0.019 (0.019)\tLoss 0.7646 (0.7496)\tPrec@1 81.250 (81.162)\n",
      "[240/391]\tTime 0.078 (0.067)\tData 0.029 (0.019)\tLoss 0.8206 (0.7497)\tPrec@1 75.781 (81.169)\n",
      "[250/391]\tTime 0.066 (0.067)\tData 0.018 (0.019)\tLoss 0.8117 (0.7500)\tPrec@1 79.688 (81.129)\n",
      "[260/391]\tTime 0.066 (0.067)\tData 0.018 (0.019)\tLoss 0.7722 (0.7479)\tPrec@1 81.250 (81.196)\n",
      "[270/391]\tTime 0.067 (0.067)\tData 0.019 (0.019)\tLoss 0.7886 (0.7471)\tPrec@1 82.812 (81.250)\n",
      "[280/391]\tTime 0.067 (0.067)\tData 0.018 (0.019)\tLoss 0.7297 (0.7472)\tPrec@1 83.594 (81.283)\n",
      "[290/391]\tTime 0.065 (0.067)\tData 0.018 (0.019)\tLoss 0.7515 (0.7470)\tPrec@1 82.812 (81.312)\n",
      "[300/391]\tTime 0.066 (0.067)\tData 0.019 (0.019)\tLoss 0.6390 (0.7471)\tPrec@1 82.031 (81.338)\n",
      "[310/391]\tTime 0.068 (0.067)\tData 0.020 (0.019)\tLoss 0.7119 (0.7457)\tPrec@1 82.812 (81.401)\n",
      "[320/391]\tTime 0.068 (0.067)\tData 0.018 (0.019)\tLoss 0.6611 (0.7461)\tPrec@1 82.812 (81.379)\n",
      "[330/391]\tTime 0.071 (0.067)\tData 0.018 (0.019)\tLoss 0.7565 (0.7460)\tPrec@1 81.250 (81.373)\n",
      "[340/391]\tTime 0.067 (0.067)\tData 0.018 (0.019)\tLoss 0.7533 (0.7464)\tPrec@1 78.125 (81.342)\n",
      "[350/391]\tTime 0.065 (0.067)\tData 0.018 (0.019)\tLoss 0.6864 (0.7468)\tPrec@1 85.156 (81.332)\n",
      "[360/391]\tTime 0.068 (0.067)\tData 0.019 (0.019)\tLoss 0.9498 (0.7464)\tPrec@1 77.344 (81.337)\n",
      "[370/391]\tTime 0.070 (0.067)\tData 0.018 (0.019)\tLoss 0.8119 (0.7465)\tPrec@1 79.688 (81.298)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[380/391]\tTime 0.066 (0.067)\tData 0.018 (0.019)\tLoss 0.6819 (0.7465)\tPrec@1 81.250 (81.312)\n",
      "[390/391]\tTime 0.048 (0.067)\tData 0.012 (0.019)\tLoss 0.5979 (0.7468)\tPrec@1 87.500 (81.314)\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.018 (0.018)\tLoss 0.8818 (0.8818)\tPrec@1 75.781 (75.781)\n",
      "Test: [10/79]\tTime 0.018 (0.017)\tLoss 1.1540 (1.1219)\tPrec@1 74.219 (74.006)\n",
      "Test: [20/79]\tTime 0.017 (0.017)\tLoss 0.9263 (1.1869)\tPrec@1 80.469 (73.251)\n",
      "Test: [30/79]\tTime 0.017 (0.017)\tLoss 0.9475 (1.1852)\tPrec@1 81.250 (73.564)\n",
      "Test: [40/79]\tTime 0.018 (0.017)\tLoss 0.9422 (1.1819)\tPrec@1 71.875 (73.819)\n",
      "Test: [50/79]\tTime 0.018 (0.017)\tLoss 1.1334 (1.1700)\tPrec@1 72.656 (74.127)\n",
      "Test: [60/79]\tTime 0.017 (0.017)\tLoss 1.2141 (1.1657)\tPrec@1 70.312 (74.219)\n",
      "Test: [70/79]\tTime 0.017 (0.017)\tLoss 0.9716 (1.1686)\tPrec@1 76.562 (74.120)\n",
      " * Prec@1 74.130\n",
      "\n",
      "===> epoch: 5/200\n",
      "current lr 1.00000e-01\n",
      "Training:\n",
      "[0/391]\tTime 0.072 (0.072)\tData 0.020 (0.020)\tLoss 0.7315 (0.7315)\tPrec@1 81.250 (81.250)\n",
      "[10/391]\tTime 0.072 (0.071)\tData 0.019 (0.019)\tLoss 0.6208 (0.6731)\tPrec@1 86.719 (82.955)\n",
      "[20/391]\tTime 0.067 (0.070)\tData 0.018 (0.019)\tLoss 0.5196 (0.6907)\tPrec@1 89.062 (83.222)\n",
      "[30/391]\tTime 0.068 (0.069)\tData 0.018 (0.019)\tLoss 0.8124 (0.7090)\tPrec@1 80.469 (82.787)\n",
      "[40/391]\tTime 0.071 (0.069)\tData 0.018 (0.019)\tLoss 0.6890 (0.7146)\tPrec@1 82.031 (82.450)\n",
      "[50/391]\tTime 0.068 (0.069)\tData 0.019 (0.019)\tLoss 0.7710 (0.7225)\tPrec@1 79.688 (82.261)\n",
      "[60/391]\tTime 0.066 (0.069)\tData 0.019 (0.019)\tLoss 0.6372 (0.7237)\tPrec@1 88.281 (82.300)\n",
      "[70/391]\tTime 0.075 (0.069)\tData 0.022 (0.019)\tLoss 0.8112 (0.7246)\tPrec@1 79.688 (82.152)\n",
      "[80/391]\tTime 0.070 (0.069)\tData 0.020 (0.019)\tLoss 0.8226 (0.7290)\tPrec@1 79.688 (82.079)\n",
      "[90/391]\tTime 0.065 (0.069)\tData 0.018 (0.019)\tLoss 0.8438 (0.7363)\tPrec@1 78.906 (81.799)\n",
      "[100/391]\tTime 0.065 (0.069)\tData 0.018 (0.019)\tLoss 0.6179 (0.7369)\tPrec@1 83.594 (81.791)\n",
      "[110/391]\tTime 0.067 (0.068)\tData 0.018 (0.019)\tLoss 0.6933 (0.7385)\tPrec@1 81.250 (81.715)\n",
      "[120/391]\tTime 0.067 (0.068)\tData 0.019 (0.019)\tLoss 0.7788 (0.7394)\tPrec@1 81.250 (81.689)\n",
      "[130/391]\tTime 0.065 (0.068)\tData 0.018 (0.019)\tLoss 0.7509 (0.7403)\tPrec@1 82.812 (81.739)\n",
      "[140/391]\tTime 0.066 (0.068)\tData 0.019 (0.019)\tLoss 0.8045 (0.7428)\tPrec@1 78.906 (81.643)\n",
      "[150/391]\tTime 0.066 (0.068)\tData 0.019 (0.019)\tLoss 0.8077 (0.7454)\tPrec@1 82.031 (81.597)\n",
      "[160/391]\tTime 0.066 (0.068)\tData 0.019 (0.019)\tLoss 0.6352 (0.7448)\tPrec@1 85.156 (81.595)\n",
      "[170/391]\tTime 0.066 (0.068)\tData 0.019 (0.019)\tLoss 0.7450 (0.7447)\tPrec@1 81.250 (81.552)\n",
      "[180/391]\tTime 0.067 (0.068)\tData 0.018 (0.019)\tLoss 0.7733 (0.7465)\tPrec@1 79.688 (81.470)\n",
      "[190/391]\tTime 0.070 (0.068)\tData 0.019 (0.019)\tLoss 0.6167 (0.7447)\tPrec@1 85.156 (81.524)\n",
      "[200/391]\tTime 0.066 (0.068)\tData 0.019 (0.019)\tLoss 0.6971 (0.7467)\tPrec@1 85.938 (81.437)\n",
      "[210/391]\tTime 0.070 (0.068)\tData 0.018 (0.019)\tLoss 0.6690 (0.7463)\tPrec@1 84.375 (81.417)\n",
      "[220/391]\tTime 0.068 (0.068)\tData 0.018 (0.019)\tLoss 0.7326 (0.7465)\tPrec@1 83.594 (81.413)\n",
      "[230/391]\tTime 0.067 (0.068)\tData 0.019 (0.019)\tLoss 0.7890 (0.7456)\tPrec@1 75.000 (81.375)\n",
      "[240/391]\tTime 0.067 (0.068)\tData 0.019 (0.019)\tLoss 0.6094 (0.7463)\tPrec@1 87.500 (81.415)\n",
      "[250/391]\tTime 0.067 (0.068)\tData 0.018 (0.019)\tLoss 0.8400 (0.7474)\tPrec@1 76.562 (81.375)\n",
      "[260/391]\tTime 0.071 (0.068)\tData 0.019 (0.019)\tLoss 0.8197 (0.7482)\tPrec@1 80.469 (81.379)\n",
      "[270/391]\tTime 0.067 (0.068)\tData 0.018 (0.019)\tLoss 0.8128 (0.7487)\tPrec@1 82.031 (81.388)\n",
      "[280/391]\tTime 0.065 (0.068)\tData 0.018 (0.019)\tLoss 0.7724 (0.7491)\tPrec@1 79.688 (81.347)\n",
      "[290/391]\tTime 0.066 (0.068)\tData 0.018 (0.019)\tLoss 0.7802 (0.7503)\tPrec@1 80.469 (81.285)\n",
      "[300/391]\tTime 0.066 (0.068)\tData 0.018 (0.019)\tLoss 0.6868 (0.7501)\tPrec@1 81.250 (81.302)\n",
      "[310/391]\tTime 0.066 (0.068)\tData 0.019 (0.019)\tLoss 0.8143 (0.7505)\tPrec@1 78.125 (81.237)\n",
      "[320/391]\tTime 0.067 (0.068)\tData 0.019 (0.019)\tLoss 0.7415 (0.7507)\tPrec@1 84.375 (81.267)\n",
      "[330/391]\tTime 0.069 (0.068)\tData 0.019 (0.019)\tLoss 0.7050 (0.7509)\tPrec@1 78.906 (81.259)\n",
      "[340/391]\tTime 0.069 (0.068)\tData 0.018 (0.019)\tLoss 0.6286 (0.7512)\tPrec@1 87.500 (81.248)\n",
      "[350/391]\tTime 0.066 (0.068)\tData 0.018 (0.019)\tLoss 0.7621 (0.7515)\tPrec@1 82.812 (81.232)\n",
      "[360/391]\tTime 0.068 (0.068)\tData 0.018 (0.019)\tLoss 0.7835 (0.7515)\tPrec@1 80.469 (81.226)\n",
      "[370/391]\tTime 0.072 (0.068)\tData 0.019 (0.019)\tLoss 0.7997 (0.7518)\tPrec@1 81.250 (81.237)\n",
      "[380/391]\tTime 0.067 (0.068)\tData 0.018 (0.019)\tLoss 0.7077 (0.7506)\tPrec@1 83.594 (81.289)\n",
      "[390/391]\tTime 0.047 (0.068)\tData 0.012 (0.019)\tLoss 0.8967 (0.7506)\tPrec@1 80.000 (81.294)\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.018 (0.018)\tLoss 1.6845 (1.6845)\tPrec@1 66.406 (66.406)\n",
      "Test: [10/79]\tTime 0.018 (0.017)\tLoss 1.8747 (1.8134)\tPrec@1 69.531 (64.347)\n",
      "Test: [20/79]\tTime 0.018 (0.018)\tLoss 1.5834 (1.8544)\tPrec@1 67.969 (64.286)\n",
      "Test: [30/79]\tTime 0.017 (0.018)\tLoss 1.6117 (1.8537)\tPrec@1 66.406 (64.390)\n",
      "Test: [40/79]\tTime 0.018 (0.018)\tLoss 2.5072 (1.8583)\tPrec@1 57.812 (64.291)\n",
      "Test: [50/79]\tTime 0.017 (0.018)\tLoss 2.0566 (1.8783)\tPrec@1 60.156 (63.710)\n",
      "Test: [60/79]\tTime 0.019 (0.018)\tLoss 1.7825 (1.8845)\tPrec@1 64.844 (63.896)\n",
      "Test: [70/79]\tTime 0.017 (0.018)\tLoss 1.9301 (1.8574)\tPrec@1 64.062 (64.195)\n",
      " * Prec@1 64.300\n",
      "\n",
      "===> epoch: 6/200\n",
      "current lr 1.00000e-01\n",
      "Training:\n",
      "[0/391]\tTime 0.072 (0.072)\tData 0.019 (0.019)\tLoss 0.5574 (0.5574)\tPrec@1 85.938 (85.938)\n",
      "[10/391]\tTime 0.069 (0.072)\tData 0.018 (0.019)\tLoss 0.6915 (0.6693)\tPrec@1 82.031 (83.310)\n",
      "[20/391]\tTime 0.067 (0.070)\tData 0.018 (0.019)\tLoss 0.8004 (0.7271)\tPrec@1 78.125 (81.213)\n",
      "[30/391]\tTime 0.069 (0.069)\tData 0.019 (0.019)\tLoss 0.6085 (0.7346)\tPrec@1 85.938 (81.149)\n",
      "[40/391]\tTime 0.068 (0.069)\tData 0.019 (0.019)\tLoss 0.8370 (0.7340)\tPrec@1 79.688 (81.612)\n",
      "[50/391]\tTime 0.067 (0.069)\tData 0.019 (0.019)\tLoss 0.6706 (0.7421)\tPrec@1 83.594 (81.250)\n",
      "[60/391]\tTime 0.069 (0.069)\tData 0.018 (0.019)\tLoss 0.6163 (0.7371)\tPrec@1 85.156 (81.532)\n",
      "[70/391]\tTime 0.065 (0.069)\tData 0.018 (0.019)\tLoss 0.7879 (0.7394)\tPrec@1 78.125 (81.547)\n",
      "[80/391]\tTime 0.067 (0.069)\tData 0.018 (0.019)\tLoss 0.7572 (0.7372)\tPrec@1 81.250 (81.520)\n",
      "[90/391]\tTime 0.072 (0.069)\tData 0.020 (0.019)\tLoss 0.6767 (0.7375)\tPrec@1 83.594 (81.465)\n",
      "[100/391]\tTime 0.069 (0.069)\tData 0.020 (0.019)\tLoss 0.7975 (0.7414)\tPrec@1 80.469 (81.304)\n",
      "[110/391]\tTime 0.070 (0.069)\tData 0.019 (0.019)\tLoss 0.8838 (0.7419)\tPrec@1 79.688 (81.363)\n",
      "[120/391]\tTime 0.066 (0.069)\tData 0.019 (0.019)\tLoss 0.6888 (0.7417)\tPrec@1 82.812 (81.476)\n",
      "[130/391]\tTime 0.066 (0.069)\tData 0.019 (0.019)\tLoss 0.9431 (0.7458)\tPrec@1 74.219 (81.304)\n",
      "[140/391]\tTime 0.068 (0.069)\tData 0.019 (0.019)\tLoss 0.5660 (0.7446)\tPrec@1 85.938 (81.394)\n",
      "[150/391]\tTime 0.071 (0.069)\tData 0.018 (0.019)\tLoss 0.6387 (0.7421)\tPrec@1 85.938 (81.441)\n",
      "[160/391]\tTime 0.067 (0.069)\tData 0.018 (0.019)\tLoss 0.6432 (0.7423)\tPrec@1 85.156 (81.464)\n",
      "[170/391]\tTime 0.065 (0.069)\tData 0.018 (0.019)\tLoss 0.7141 (0.7453)\tPrec@1 81.250 (81.428)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "from visdom import Visdom\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from utils import *\n",
    "from metric.loss import RKdAngle, RkdDistance\n",
    "\n",
    "# Teacher models:\n",
    "# VGG11/VGG13/VGG16/VGG19, GoogLeNet, AlxNet, ResNet18, ResNet34, \n",
    "# ResNet50, ResNet101, ResNet152, ResNeXt29_2x64d, ResNeXt29_4x64d, \n",
    "# ResNeXt29_8x64d, ResNeXt29_32x64d, PreActResNet18, PreActResNet34, \n",
    "# PreActResNet50, PreActResNet101, PreActResNet152, \n",
    "# DenseNet121, DenseNet161, DenseNet169, DenseNet201, \n",
    "import models\n",
    "\n",
    "# Student models:\n",
    "# myNet, LeNet, FitNet\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch Distill Example')\n",
    "\n",
    "parser.add_argument('--dataset',\n",
    "                    choices=['CIFAR10',\n",
    "                             'CIFAR100'\n",
    "                            ],\n",
    "                    default='CIFAR10')\n",
    "parser.add_argument('--teachers',\n",
    "                    default=['ResNet32', 'ResNet56', 'ResNet110'],\n",
    "                    nargs='+')\n",
    "parser.add_argument('--student',\n",
    "                    choices=['ResNet8',\n",
    "                             'ResNet15',\n",
    "                             'ResNet20',\n",
    "                             'myNet'\n",
    "                            ],\n",
    "                    default='ResNet8')\n",
    "parser.add_argument('--kd_ratio', default=0.7, type=float)\n",
    "parser.add_argument('--n_class', type=int, default=10, metavar='N', help='num of classes')\n",
    "parser.add_argument('--T', type=float, default=20.0, metavar='Temputure', help='Temputure for distillation')\n",
    "parser.add_argument('--batch_size', type=int, default=128, metavar='N', help='input batch size for training')\n",
    "parser.add_argument('--test_batch_size', type=int, default=128, metavar='N', help='input test batch size for training')\n",
    "parser.add_argument('--epochs', type=int, default=20, metavar='N', help='number of epochs to train (default: 20)')\n",
    "parser.add_argument('--lr', type=float, default=0.1, metavar='LR', help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, metavar='M', help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--device', default='cuda:1', type=str, help='device: cuda or cpu')\n",
    "parser.add_argument('--print_freq', type=int, default=10, metavar='N', help='how many batches to wait before logging training status')\n",
    "\n",
    "config = ['--dataset', 'CIFAR10', '--epochs', '200', '--T', '5.0', '--n_class', '10', '--device', 'cuda:0']\n",
    "args = parser.parse_args(config)\n",
    "\n",
    "device = args.device if torch.cuda.is_available() else 'cpu'\n",
    "load_dir = './checkpoint/' + args.dataset + '/'\n",
    "\n",
    "# teachers model\n",
    "teacher_models = []\n",
    "for te in args.teachers:\n",
    "    te_model = getattr(models, te)(num_classes=args.n_class)\n",
    "#     print(te_model)\n",
    "    te_model.load_state_dict(torch.load(load_dir + te_model.model_name + '.pth'))\n",
    "    te_model.to(device)\n",
    "    teacher_models.append(te_model)\n",
    "\n",
    "st_model = getattr(models, args.student)(num_classes=args.n_class)  # args.student()\n",
    "st_model.to(device)\n",
    "\n",
    "# logging\n",
    "logfile = load_dir + 'adapter_distill_' + st_model.model_name + '.log'\n",
    "if os.path.exists(logfile):\n",
    "    os.remove(logfile)\n",
    "def log_out(info):\n",
    "    f = open(logfile, mode='a')\n",
    "    f.write(info)\n",
    "    f.write('\\n')\n",
    "    f.close()\n",
    "    print(info)\n",
    "    \n",
    "# visualizer\n",
    "vis = Visdom(env='distill')\n",
    "loss_win = vis.line(\n",
    "    X=np.array([0]),\n",
    "    Y=np.array([0]),\n",
    "    opts=dict(\n",
    "        title='multi ada. train loss',\n",
    "        xlabel='epoch',\n",
    "        xtickmin=0,\n",
    "#         xtickmax=1,\n",
    "        ylabel='loss',\n",
    "        ytickmin=0,\n",
    "#         ytickmax=1,\n",
    "        ytickstep=0.5,\n",
    "#         markers=True,\n",
    "#         markersymbol='dot',\n",
    "#         markersize=5,\n",
    "    ),\n",
    "    name=\"loss\"\n",
    ")\n",
    "\n",
    "acc_win = vis.line(\n",
    "    X=np.column_stack((0, 0)),\n",
    "    Y=np.column_stack((0, 0)),\n",
    "    opts=dict(\n",
    "        title='multi-KD ada. ACC',\n",
    "        xlabel='epoch',\n",
    "        xtickmin=0,\n",
    "        ylabel='accuracy',\n",
    "        ytickmin=0,\n",
    "        ytickmax=100,\n",
    "#         markers=True,\n",
    "#         markersymbol='dot',\n",
    "#         markersize=5,\n",
    "        legend=['train_acc', 'test_acc']\n",
    "    ),\n",
    "    name=\"acc\"\n",
    ")\n",
    "\n",
    "\n",
    "# adapter model\n",
    "class Adapter():\n",
    "    def __init__(self, in_models, pool_size):\n",
    "        # representations of teachers\n",
    "        pool_ch = pool_size[1]  # 64\n",
    "        pool_w = pool_size[2]   # 8\n",
    "        LR_list = []\n",
    "        torch.manual_seed(1)\n",
    "        self.theta = torch.randn(len(in_models), pool_ch).to(device)  # [3, 64]\n",
    "        self.theta.requires_grad_(True)\n",
    "   \n",
    "        self.max_feat = nn.MaxPool2d(kernel_size=(pool_w, pool_w), stride=pool_w).to(device)\n",
    "        self.W = torch.randn(pool_ch, 1).to(device)\n",
    "        self.W.requires_grad_(True)\n",
    "        self.val = False\n",
    "\n",
    "    def loss(self, y, labels, weighted_logits, T=10.0, alpha=0.7):\n",
    "        ls = nn.KLDivLoss()(F.log_softmax(y/T), weighted_logits) * (T*T * 2.0 * alpha) + F.cross_entropy(y, labels) * (1. - alpha)\n",
    "        if not self.val:\n",
    "            ls += 0.1 * (torch.sum(self.W * self.W) + torch.sum(torch.sum(self.theta * self.theta, dim=1), dim=0))\n",
    "        return ls\n",
    "        \n",
    "    def gradient(self, lr=0.01):\n",
    "        self.W.data = self.W.data - lr * self.W.grad.data\n",
    "        # Manually zero the gradients after updating weights\n",
    "        self.W.grad.data.zero_()\n",
    "        \n",
    "    def eval(self):\n",
    "        self.val = True\n",
    "        self.theta.detach()\n",
    "        self.W.detach()\n",
    "    \n",
    "    # input size: [64, 8, 8], [128, 3, 10]\n",
    "    def forward(self, conv_map, te_logits_list):\n",
    "        beta = self.max_feat(conv_map)\n",
    "        beta = torch.squeeze(beta)  # [128, 64]\n",
    "        \n",
    "        latent_factor = []\n",
    "        for t in self.theta:\n",
    "            latent_factor.append(beta * t)\n",
    "#         latent_factor = torch.stack(latent_factor, dim=0)  # [3, 128, 64]\n",
    "        alpha = []\n",
    "        for lf in latent_factor:  # lf.size:[128, 64]\n",
    "            alpha.append(lf.mm(self.W))\n",
    "        alpha = torch.stack(alpha, dim=0)  # [3, 128, 1]\n",
    "        alpha = torch.squeeze(alpha).transpose(0, 1) # [128, 3]\n",
    "        miu = F.softmax(alpha)  # [128, 3]\n",
    "        miu = torch.unsqueeze(miu, dim=2)\n",
    "        weighted_logits = miu * te_logits_list  # [128, 3, 10]\n",
    "        weighted_logits = torch.sum(weighted_logits, dim=1)\n",
    "#         print(weighted_logits)\n",
    "        \n",
    "        return weighted_logits\n",
    "\n",
    "# adapter instance\n",
    "_,_,_,pool_m,_ = st_model(torch.randn(1,3, 128, 128).to(device))  # get pool_size of student\n",
    "# reate adapter instance\n",
    "adapter = Adapter(teacher_models, pool_m.size())\n",
    "\n",
    "\n",
    "# data\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, 4),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "test_transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "train_set = getattr(datasets, args.dataset)(root='../data', train=True, download=True, transform=train_transform)\n",
    "test_set = getattr(datasets, args.dataset)(root='../data', train=False, download=False, transform=test_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=args.test_batch_size, shuffle=False)\n",
    "# optim\n",
    "optimizer_W = optim.SGD([adapter.W], lr=args.lr, momentum=0.9)\n",
    "optimizer_theta = optim.SGD([adapter.theta], lr=args.lr, momentum=0.9)\n",
    "optimizer_sgd = optim.SGD(st_model.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
    "lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_sgd, gamma=0.1, milestones=[100, 150])\n",
    "lr_scheduler2 = optim.lr_scheduler.MultiStepLR(optimizer_W, milestones=[40, 50])\n",
    "lr_scheduler3 = optim.lr_scheduler.MultiStepLR(optimizer_theta, milestones=[40, 50])\n",
    "\n",
    "# attention transfer loss\n",
    "dist_criterion = RkdDistance().to(device)\n",
    "angle_criterion = RKdAngle().to(device)\n",
    "\n",
    "\n",
    "def train_adapter(n_epochs=70, model=st_model):\n",
    "    print('Training adapter:')\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    adapter.eval()\n",
    "    for ep in range(n_epochs):\n",
    "        lr_scheduler2.step()\n",
    "        lr_scheduler3.step()\n",
    "        for i, (input, target) in enumerate(train_loader):\n",
    "\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            # compute outputs\n",
    "            b1, b2, b3, pool, output = model(input) # out_feat: 16, 32, 64, 64, - \n",
    "#             print('b1:{}, b2:{}, b3{}, pool:{}'.format(b1.size(), b2.size(), b3.size(), pool.size()))\n",
    "\n",
    "            te_scores_list = []\n",
    "            for j,te in enumerate(teacher_models):\n",
    "                te.eval()\n",
    "                with torch.no_grad():\n",
    "                    t_b1, t_b2, t_b3, t_pool, t_output = te(input)\n",
    "#                 print('t_b1:{}, t_b2:{}, t_b3{}, t_pool:{}'.format(t_b1.size(), t_b2.size(), t_b3.size(), t_pool.size()))\n",
    "                t_output = F.softmax(t_output/args.T)\n",
    "                te_scores_list.append(t_output)\n",
    "            te_scores_Tensor = torch.stack(te_scores_list, dim=1)  # size: [128, 3, 10]\n",
    "            \n",
    "            optimizer_sgd.zero_grad()\n",
    "            optimizer_W.zero_grad()\n",
    "            optimizer_theta.zero_grad()\n",
    "            \n",
    "            weighted_logits = adapter.forward(pool, te_scores_Tensor)\n",
    "            \n",
    "            angle_loss = angle_criterion(output, weighted_logits)\n",
    "            dist_loss = dist_criterion(output, weighted_logits)\n",
    "            # compute gradient and do SGD step\n",
    "            ada_loss = adapter.loss(output, target, weighted_logits, T=args.T, alpha=args.kd_ratio)\n",
    "            loss = ada_loss + angle_loss + dist_loss\n",
    "            \n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer_sgd.step()\n",
    "            optimizer_W.step()\n",
    "            optimizer_theta.step()\n",
    "            \n",
    "#          vis.line(np.array([loss.item()]), np.array([ep]), loss_win, update=\"append\")\n",
    "        log_out('epoch[{}/{}]adapter Loss: {:.4f}'.format(ep, n_epochs, loss.item()))\n",
    "    end_time = time.time()\n",
    "    log_out(\"--- adapter training cost {:.3f} mins ---\".format((end_time - start_time)/60))\n",
    "\n",
    "\n",
    "# train with multi-teacher\n",
    "def train(epoch, model):\n",
    "    print('Training:')\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    adapter.eval()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    \n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        \n",
    "        # compute outputs\n",
    "        b1, b2, b3, pool, output = model(input)\n",
    "        \n",
    "        te_scores_list = []\n",
    "        for j,te in enumerate(teacher_models):\n",
    "            te.eval()\n",
    "            with torch.no_grad():\n",
    "            t_b1, t_b2, t_b3, t_pool, t_output = te(input)\n",
    "                t_output = F.softmax(t_output/args.T)\n",
    "            te_scores_list.append(t_output)\n",
    "        te_scores_Tensor = torch.stack(te_scores_list, dim=1)  # size: [128, 3, 10]\n",
    "        weighted_logits = adapter.forward(pool, te_scores_Tensor)\n",
    "        \n",
    "        optimizer_sgd.zero_grad()\n",
    "        \n",
    "        angle_loss = angle_criterion(output, weighted_logits)\n",
    "        dist_loss = dist_criterion(output, weighted_logits)\n",
    "\n",
    "        weighted_logits = adapter.forward(pool, te_scores_Tensor)\n",
    "        # compute gradient and do SGD step\n",
    "        ada_loss = adapter.loss(output, target, weighted_logits, T=args.T, alpha=args.kd_ratio)\n",
    "        loss = ada_loss + angle_loss + dist_loss\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer_sgd.step()\n",
    "\n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "        # measure accuracy and record loss\n",
    "        train_acc = accuracy(output.data, target.data)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(train_acc, input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            log_out('[{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      i, len(train_loader), batch_time=batch_time,\n",
    "                      data_time=data_time, loss=losses, top1=top1))\n",
    "    return losses.avg, train_acc.cpu().numpy()\n",
    "\n",
    "\n",
    "def test(model):\n",
    "    print('Testing:')\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(test_loader):\n",
    "            input, target = input.to(device), target.to(device)\n",
    "\n",
    "            # compute output\n",
    "            _,_,_,_,output = model(input)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "\n",
    "            output = output.float()\n",
    "            loss = loss.float()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            test_acc = accuracy(output.data, target.data)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(test_acc, input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % args.print_freq == 0:\n",
    "                log_out('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                          i, len(test_loader), batch_time=batch_time, loss=losses,\n",
    "                          top1=top1))\n",
    "\n",
    "    log_out(' * Prec@1 {top1.avg:.3f}'.format(top1=top1))\n",
    "\n",
    "    return losses.avg, test_acc.cpu().numpy(), top1.avg.cpu().numpy()\n",
    "\n",
    "# \"\"\"\n",
    "print('StudentNet:\\n')\n",
    "print(st_model)\n",
    "st_model.apply(weights_init_normal)\n",
    "train_adapter(n_epochs=80)\n",
    "# st_model.apply(weights_init_normal)\n",
    "best_acc = 0\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    log_out(\"\\n===> epoch: {}/{}\".format(epoch, args.epochs))\n",
    "    log_out('current lr {:.5e}'.format(optimizer_sgd.param_groups[0]['lr']))\n",
    "    lr_scheduler.step(epoch)\n",
    "    train_loss, train_acc = train(epoch, st_model)\n",
    "    # visaulize loss\n",
    "    vis.line(np.array([train_loss]), np.array([epoch]), loss_win, update=\"append\")\n",
    "    _, test_acc, top1 = test(st_model)\n",
    "    vis.line(np.column_stack((train_acc, top1)), np.column_stack((epoch, epoch)), acc_win, update=\"append\")\n",
    "    if top1 > best_acc:\n",
    "        best_acc = top1\n",
    "        \n",
    "# release GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "log_out(\"BEST ACC: {:.3f}\".format(best_acc))\n",
    "log_out(\"--- {:.3f} mins ---\".format((time.time() - start_time)/60))\n",
    "# \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
