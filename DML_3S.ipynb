{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-03T07:35:22.228272Z",
     "start_time": "2019-08-03T07:32:19.281366Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "*-----------------DML----------------*\n",
      "\n",
      "===> epoch: 1/200\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaliu/Dev/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:160: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/391]\tTime 0.343 (0.343)\tData 0.019 (0.019)\tLoss 6.4467 (4.5754)\tPrec@1 (7.031)\n",
      "[0/391]\tTime 0.343 (0.343)\tData 0.019 (0.019)\tLoss 6.4467 (4.5754)\tPrec@1 (7.031)\n",
      "[0/391]\tTime 0.343 (0.343)\tData 0.019 (0.019)\tLoss 6.4467 (4.5754)\tPrec@1 (7.031)\n",
      "[40/391]\tTime 0.317 (0.319)\tData 0.018 (0.018)\tLoss 2.3183 (2.7125)\tPrec@1 (13.993)\n",
      "[40/391]\tTime 0.317 (0.319)\tData 0.018 (0.018)\tLoss 2.3183 (2.7125)\tPrec@1 (13.993)\n",
      "[40/391]\tTime 0.317 (0.319)\tData 0.018 (0.018)\tLoss 2.3183 (2.7125)\tPrec@1 (13.993)\n",
      "[80/391]\tTime 0.328 (0.319)\tData 0.018 (0.018)\tLoss 1.9819 (2.3065)\tPrec@1 (16.917)\n",
      "[80/391]\tTime 0.328 (0.319)\tData 0.018 (0.018)\tLoss 1.9819 (2.3065)\tPrec@1 (16.917)\n",
      "[80/391]\tTime 0.328 (0.319)\tData 0.018 (0.018)\tLoss 1.9819 (2.3065)\tPrec@1 (16.917)\n",
      "[120/391]\tTime 0.328 (0.320)\tData 0.018 (0.018)\tLoss 1.9261 (2.1333)\tPrec@1 (19.551)\n",
      "[120/391]\tTime 0.328 (0.320)\tData 0.018 (0.018)\tLoss 1.9261 (2.1333)\tPrec@1 (19.551)\n",
      "[120/391]\tTime 0.328 (0.320)\tData 0.018 (0.018)\tLoss 1.9261 (2.1333)\tPrec@1 (19.551)\n",
      "[160/391]\tTime 0.327 (0.323)\tData 0.018 (0.018)\tLoss 1.8759 (2.0296)\tPrec@1 (21.610)\n",
      "[160/391]\tTime 0.327 (0.323)\tData 0.018 (0.018)\tLoss 1.8759 (2.0296)\tPrec@1 (21.610)\n",
      "[160/391]\tTime 0.327 (0.323)\tData 0.018 (0.018)\tLoss 1.8759 (2.0296)\tPrec@1 (21.610)\n",
      "[200/391]\tTime 0.334 (0.325)\tData 0.018 (0.018)\tLoss 1.7957 (1.9522)\tPrec@1 (23.312)\n",
      "[200/391]\tTime 0.334 (0.325)\tData 0.018 (0.018)\tLoss 1.7957 (1.9522)\tPrec@1 (23.312)\n",
      "[200/391]\tTime 0.334 (0.325)\tData 0.018 (0.018)\tLoss 1.7957 (1.9522)\tPrec@1 (23.312)\n",
      "[240/391]\tTime 0.352 (0.329)\tData 0.019 (0.018)\tLoss 1.7561 (1.8937)\tPrec@1 (24.799)\n",
      "[240/391]\tTime 0.352 (0.329)\tData 0.019 (0.018)\tLoss 1.7561 (1.8937)\tPrec@1 (24.799)\n",
      "[240/391]\tTime 0.352 (0.329)\tData 0.019 (0.018)\tLoss 1.7561 (1.8937)\tPrec@1 (24.799)\n",
      "[280/391]\tTime 0.379 (0.332)\tData 0.018 (0.018)\tLoss 1.6914 (1.8409)\tPrec@1 (26.356)\n",
      "[280/391]\tTime 0.379 (0.332)\tData 0.018 (0.018)\tLoss 1.6914 (1.8409)\tPrec@1 (26.356)\n",
      "[280/391]\tTime 0.379 (0.332)\tData 0.018 (0.018)\tLoss 1.6914 (1.8409)\tPrec@1 (26.356)\n",
      "[320/391]\tTime 0.336 (0.333)\tData 0.018 (0.018)\tLoss 1.6142 (1.7941)\tPrec@1 (27.836)\n",
      "[320/391]\tTime 0.336 (0.333)\tData 0.018 (0.018)\tLoss 1.6142 (1.7941)\tPrec@1 (27.836)\n",
      "[320/391]\tTime 0.336 (0.333)\tData 0.018 (0.018)\tLoss 1.6142 (1.7941)\tPrec@1 (27.836)\n",
      "[360/391]\tTime 0.333 (0.335)\tData 0.018 (0.018)\tLoss 1.6891 (1.7526)\tPrec@1 (29.186)\n",
      "[360/391]\tTime 0.333 (0.335)\tData 0.018 (0.018)\tLoss 1.6891 (1.7526)\tPrec@1 (29.186)\n",
      "[360/391]\tTime 0.333 (0.335)\tData 0.018 (0.018)\tLoss 1.6891 (1.7526)\tPrec@1 (29.186)\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.019 (0.019)\tLoss 1.4136 (1.4136)\tPrec@1 46.094 (46.094)\n",
      "Test: [40/79]\tTime 0.019 (0.018)\tLoss 1.2804 (1.3270)\tPrec@1 54.688 (51.086)\n",
      " * ResNet20 Prec@1 50.810\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.026 (0.026)\tLoss 1.7197 (1.7197)\tPrec@1 35.938 (35.938)\n",
      "Test: [40/79]\tTime 0.025 (0.025)\tLoss 1.7212 (1.7040)\tPrec@1 34.375 (37.024)\n",
      " * ResNet56 Prec@1 36.260\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.035 (0.035)\tLoss 1.7451 (1.7451)\tPrec@1 39.844 (39.844)\n",
      "Test: [40/79]\tTime 0.033 (0.034)\tLoss 1.7589 (1.7887)\tPrec@1 31.250 (32.393)\n",
      " * ResNet110 Prec@1 32.120\n",
      "\n",
      "===> epoch: 2/200\n",
      "Training:\n",
      "[0/391]\tTime 0.354 (0.354)\tData 0.033 (0.033)\tLoss 1.7618 (1.5796)\tPrec@1 (37.760)\n",
      "[0/391]\tTime 0.354 (0.354)\tData 0.033 (0.033)\tLoss 1.7618 (1.5796)\tPrec@1 (37.760)\n",
      "[0/391]\tTime 0.354 (0.354)\tData 0.033 (0.033)\tLoss 1.7618 (1.5796)\tPrec@1 (37.760)\n",
      "[40/391]\tTime 0.334 (0.343)\tData 0.018 (0.019)\tLoss nan (nan)\tPrec@1 (7.539)\n",
      "[40/391]\tTime 0.334 (0.343)\tData 0.018 (0.019)\tLoss nan (nan)\tPrec@1 (7.539)\n",
      "[40/391]\tTime 0.334 (0.343)\tData 0.018 (0.019)\tLoss nan (nan)\tPrec@1 (7.539)\n",
      "[80/391]\tTime 0.357 (0.346)\tData 0.018 (0.019)\tLoss nan (nan)\tPrec@1 (7.115)\n",
      "[80/391]\tTime 0.357 (0.346)\tData 0.018 (0.019)\tLoss nan (nan)\tPrec@1 (7.115)\n",
      "[80/391]\tTime 0.357 (0.346)\tData 0.018 (0.019)\tLoss nan (nan)\tPrec@1 (7.115)\n",
      "[120/391]\tTime 0.322 (0.348)\tData 0.018 (0.018)\tLoss nan (nan)\tPrec@1 (6.891)\n",
      "[120/391]\tTime 0.322 (0.348)\tData 0.018 (0.018)\tLoss nan (nan)\tPrec@1 (6.891)\n",
      "[120/391]\tTime 0.322 (0.348)\tData 0.018 (0.018)\tLoss nan (nan)\tPrec@1 (6.891)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-daa768336c94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mlr_scheduler_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop1_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnets_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;31m# visaulize loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_win\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-daa768336c94>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, nets_list)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mloss_j\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mloss_j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0moptimizers_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;31m# measure accuracy and record loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/py37/lib/python3.7/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                         \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "from visdom import Visdom\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from utils import *\n",
    "\n",
    "# Teacher models:\n",
    "# VGG11/VGG13/VGG16/VGG19, GoogLeNet, AlxNet, ResNet18, ResNet34, \n",
    "# ResNet50, ResNet101, ResNet152, ResNeXt29_2x64d, ResNeXt29_4x64d, \n",
    "# ResNeXt29_8x64d, ResNeXt29_32x64d, PreActResNet18, PreActResNet34, \n",
    "# PreActResNet50, PreActResNet101, PreActResNet152, \n",
    "# DenseNet121, DenseNet161, DenseNet169, DenseNet201, \n",
    "import models\n",
    "\n",
    "# Student models:\n",
    "# myNet, LeNet, FitNet\n",
    "\n",
    "start_time = time.time()\n",
    "# os.makedirs('./checkpoint', exist_ok=True)\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch DML 2S')\n",
    "\n",
    "parser.add_argument('--dataset',\n",
    "                    choices=['CIFAR10',\n",
    "                             'CIFAR100'\n",
    "                            ],\n",
    "                    default='CIFAR10')\n",
    "parser.add_argument('--nets',\n",
    "                    choices=['ResNet32',\n",
    "                             'ResNet50',\n",
    "                             'ResNet56',\n",
    "                             'ResNet110'\n",
    "                            ],\n",
    "                    default=['ResNet20', 'ResNet56', 'ResNet110'],\n",
    "                    nargs='+')\n",
    "\n",
    "parser.add_argument('--n_class', type=int, default=10, metavar='N', help='num of classes')\n",
    "parser.add_argument('--batch_size', type=int, default=128, metavar='N', help='input batch size for training')\n",
    "parser.add_argument('--test_batch_size', type=int, default=128, metavar='N', help='input test batch size for training')\n",
    "parser.add_argument('--epochs', type=int, default=20, metavar='N', help='number of epochs to train (default: 20)')\n",
    "parser.add_argument('--lr', type=float, default=0.1, metavar='LR', help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, metavar='M', help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--device', default='cuda:1', type=str, help='device: cuda or cpu')\n",
    "parser.add_argument('--print_freq', type=int, default=40, metavar='N', help='how many batches to wait before logging training status')\n",
    "\n",
    "config = ['--epochs', '200', '--device', 'cuda:1']\n",
    "args = parser.parse_args(config)\n",
    "\n",
    "device = args.device if torch.cuda.is_available() else 'cpu'\n",
    "save_dir = './checkpoint/' + args.dataset + '/'\n",
    "\n",
    "# models\n",
    "nets_list = []\n",
    "for m in args.nets:\n",
    "    net = getattr(models, m)()\n",
    "    net.to(device)\n",
    "    net.train()  # train mode\n",
    "    nets_list.append(net)\n",
    "\n",
    "K = len(nets_list)\n",
    "    \n",
    "# logging\n",
    "logfile = save_dir + 'DML_3S_.log'\n",
    "if os.path.exists(logfile):\n",
    "    os.remove(logfile)\n",
    "def log_out(info):\n",
    "    f = open(logfile, mode='a')\n",
    "    f.write(info)\n",
    "    f.write('\\n')\n",
    "    f.close()\n",
    "    print(info)\n",
    "    \n",
    "# visualizer\n",
    "vis = Visdom(env='distill')\n",
    "loss_win = vis.line(\n",
    "    X=np.array([0]*K),\n",
    "    Y=np.array([0]*K),\n",
    "    opts=dict(\n",
    "        title='DML_3S Loss',\n",
    "        xlabel='epoch',\n",
    "        xtickmin=0,\n",
    "        ylabel='loss',\n",
    "        ytickmin=0,\n",
    "        ytickstep=0.5\n",
    "    ),\n",
    "    name=\"loss\"\n",
    ")\n",
    "\n",
    "acc_win = vis.line(\n",
    "    X=np.array([0]*K),\n",
    "    Y=np.array([0]*K),\n",
    "    opts=dict(\n",
    "        title='DML_3S Acc',\n",
    "        xlabel='epoch',\n",
    "        xtickmin=0,\n",
    "        ylabel='accuracy',\n",
    "        ytickmin=0,\n",
    "        ytickmax=100\n",
    "    ),\n",
    "    name=\"acc\"\n",
    ")\n",
    "\n",
    "\n",
    "# data\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, 4),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "test_transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "train_set = datasets.CIFAR10(root='../data', train=True, download=True, transform=train_transform)\n",
    "test_set = datasets.CIFAR10(root='../data', train=False, download=False, transform=test_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=args.test_batch_size, shuffle=False)\n",
    "\n",
    "# optimizer = optim.SGD(st_model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "optimizers_list = []\n",
    "lr_scheduler_list = []\n",
    "for m in nets_list:\n",
    "    optimizer_m = optim.SGD(m.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
    "    lr_scheduler_m = optim.lr_scheduler.MultiStepLR(optimizer_m, milestones=[100, 150])\n",
    "    optimizers_list.append(optimizer_m)\n",
    "    lr_scheduler_list.append(lr_scheduler_m)\n",
    "\n",
    "    \n",
    "# train with multi-teacher\n",
    "def train(epoch, nets_list):\n",
    "    print('Training:')\n",
    "    K = len(nets_list)\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses_list = [AverageMeter()] * K\n",
    "    top1_list = [AverageMeter()] * K\n",
    "    \n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        \n",
    "        # compute outputs\n",
    "        output_list = []\n",
    "        logits_list = []\n",
    "        for net in nets_list:\n",
    "            _,_,_,_, output_m = net(input)\n",
    "            logits_m = F.softmax(output_m)\n",
    "            output_list.append(output_m)\n",
    "            logits_list.append(logits_m)\n",
    "        \n",
    "        for j in range(K):\n",
    "            loss_j = 0\n",
    "            \n",
    "            optimizers_list[j].zero_grad()\n",
    "            for h in range(K):\n",
    "                if h != j:\n",
    "                    loss_j += nn.KLDivLoss()(logits_list[h], logits_list[j]) \n",
    "            loss_j /= K - 1\n",
    "            loss_j += F.cross_entropy(output_list[j], target)\n",
    "            loss_j.backward()  # retain_graph=True\n",
    "            optimizers_list[j].step()\n",
    "            \n",
    "            # measure accuracy and record loss\n",
    "            netj_acc = accuracy(output_list[j], target)[0]\n",
    "            losses_list[j].update(loss_j.item(), input.size(0))\n",
    "            top1_list[j].update(netj_acc, input.size(0))\n",
    "        \n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            for j in range(K):\n",
    "                log_out('[{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Prec@1 ({top1_1.avg:.3f})'.format(\n",
    "                          i, len(train_loader), batch_time=batch_time,\n",
    "                          data_time=data_time, loss=losses_list[j], top1_1=top1_list[j]))\n",
    "    \n",
    "    losses_list = [losses_list[j].avg for j in range(K)]\n",
    "    top1_list = [top1_list[j].avg.cpu().numpy() for j in range(K)]\n",
    "    \n",
    "    return losses_list, top1_list\n",
    "\n",
    "\n",
    "def test(model):\n",
    "    print('Testing:')\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(test_loader):\n",
    "            input, target = input.to(device), target.to(device)\n",
    "\n",
    "            # compute output\n",
    "            _,_,_,_,output = model(input)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "\n",
    "            output = output.float()\n",
    "            loss = loss.float()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            test_acc = accuracy(output.data, target.data)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(test_acc, input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % args.print_freq == 0:\n",
    "                log_out('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                          i, len(test_loader), batch_time=batch_time, loss=losses,\n",
    "                          top1=top1))\n",
    "\n",
    "    log_out(' * {0} Prec@1 {top1.avg:.3f}'.format(model.model_name, top1=top1))\n",
    "\n",
    "    return losses.avg, test_acc.cpu().numpy(), top1.avg.cpu().numpy()\n",
    "\n",
    "\n",
    "print('*-----------------DML----------------*')\n",
    "best_acc_list = [0] * K\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    log_out(\"\\n===> epoch: {}/{}\".format(epoch, args.epochs))\n",
    "#     log_out('current lr {:.5e}'.format(optimizer_1.param_groups[0]['lr']))\n",
    "    for j in range(K):\n",
    "        lr_scheduler_list[j].step()\n",
    "    train_loss_list, top1_list = train(epoch, nets_list)\n",
    "    # visaulize loss\n",
    "    vis.line(np.column_stack(np.array(train_loss_list)), np.column_stack((epoch) * K), loss_win, update=\"append\")\n",
    "    top1_list = []\n",
    "    for j in range(K):\n",
    "        _, _, top1 = test(nets_list[j])\n",
    "        best_acc_list[j] = max(top1, best_acc_list[j])\n",
    "        top1_list.append(top1)\n",
    "    \n",
    "    vis.line(np.column_stack(np.array(top1_list))),  np.column_stack((epoch) * K), acc_win, update=\"append\")\n",
    "    \n",
    "for j in range(K):\n",
    "    log_out(\"@ [{}] BEST Prec: {:.4f}\".format(nets_list[j].model_name, best_acc_list[j]))\n",
    "log_out(\"--- {:.3f} mins ---\".format((time.time() - start_time)/60))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
