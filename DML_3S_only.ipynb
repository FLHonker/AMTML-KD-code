{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-24T00:53:07.235Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/data/yaliu/jupyterbooks/multi-KD/models/teacher/resnet20.py:36: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(m.weight)\n",
      "/home/data/yaliu/jupyterbooks/multi-KD/models/student/resnet_s.py:13: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(m.weight)\n",
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "*-----------------DML----------------*\n",
      "\n",
      "===> epoch: 1/200\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaliu/Dev/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:192: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/yaliu/Dev/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:193: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/yaliu/Dev/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:194: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/yaliu/Dev/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/functional.py:1992: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/391]\tTime 0.690 (0.690)\tData 0.036 (0.036)\tLoss 8.4445 (8.4445)\tPrec@1 (0.000)\n",
      "[40/391]\tTime 0.269 (0.280)\tData 0.018 (0.019)\tLoss 2.0694 (3.1600)\tPrec@1 (14.634)\n",
      "[80/391]\tTime 0.270 (0.275)\tData 0.018 (0.019)\tLoss 1.9104 (2.5826)\tPrec@1 (19.502)\n",
      "[120/391]\tTime 0.271 (0.273)\tData 0.019 (0.019)\tLoss 1.7678 (2.3481)\tPrec@1 (22.546)\n",
      "[160/391]\tTime 0.270 (0.273)\tData 0.018 (0.019)\tLoss 1.6953 (2.2107)\tPrec@1 (24.845)\n",
      "[200/391]\tTime 0.274 (0.273)\tData 0.019 (0.019)\tLoss 1.6655 (2.1096)\tPrec@1 (26.959)\n",
      "[240/391]\tTime 0.273 (0.273)\tData 0.019 (0.019)\tLoss 1.6604 (2.0331)\tPrec@1 (28.524)\n",
      "[280/391]\tTime 0.273 (0.273)\tData 0.018 (0.019)\tLoss 1.8898 (1.9754)\tPrec@1 (29.896)\n",
      "[320/391]\tTime 0.274 (0.273)\tData 0.019 (0.019)\tLoss 1.4203 (1.9264)\tPrec@1 (31.128)\n",
      "[360/391]\tTime 0.275 (0.273)\tData 0.019 (0.019)\tLoss 1.4795 (1.8835)\tPrec@1 (32.354)\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.036 (0.036)\tLoss 1.6694 (1.6694)\tPrec@1 36.719 (36.719)\n",
      "Test: [40/79]\tTime 0.035 (0.035)\tLoss 1.6807 (1.5549)\tPrec@1 39.844 (44.341)\n",
      " * ResNet110 Prec@1 43.530\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.021 (0.021)\tLoss 1.3571 (1.3571)\tPrec@1 53.125 (53.125)\n",
      "Test: [40/79]\tTime 0.021 (0.021)\tLoss 1.4407 (1.3831)\tPrec@1 53.906 (51.143)\n",
      " * ResNet32 Prec@1 50.920\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.019 (0.019)\tLoss 1.5081 (1.5081)\tPrec@1 47.656 (47.656)\n",
      "Test: [40/79]\tTime 0.019 (0.019)\tLoss 1.5259 (1.4987)\tPrec@1 45.312 (47.732)\n",
      " * ResNet16 Prec@1 48.030\n",
      "\n",
      "===> epoch: 2/200\n",
      "Training:\n",
      "[0/391]\tTime 0.287 (0.287)\tData 0.020 (0.020)\tLoss 1.3713 (1.3713)\tPrec@1 (46.875)\n",
      "[40/391]\tTime 0.276 (0.276)\tData 0.019 (0.020)\tLoss 1.3542 (1.4587)\tPrec@1 (46.037)\n",
      "[80/391]\tTime 0.276 (0.276)\tData 0.019 (0.019)\tLoss 1.3746 (1.4472)\tPrec@1 (46.566)\n",
      "[120/391]\tTime 0.276 (0.276)\tData 0.019 (0.019)\tLoss 1.4416 (1.4179)\tPrec@1 (47.676)\n",
      "[160/391]\tTime 0.278 (0.276)\tData 0.020 (0.019)\tLoss 1.2504 (1.4135)\tPrec@1 (47.714)\n",
      "[200/391]\tTime 0.279 (0.276)\tData 0.020 (0.020)\tLoss 1.4498 (1.4076)\tPrec@1 (47.994)\n",
      "[240/391]\tTime 0.280 (0.277)\tData 0.020 (0.020)\tLoss 1.3854 (1.3968)\tPrec@1 (48.399)\n",
      "[280/391]\tTime 0.280 (0.277)\tData 0.021 (0.020)\tLoss 1.1971 (1.3819)\tPrec@1 (48.963)\n",
      "[320/391]\tTime 0.279 (0.277)\tData 0.020 (0.020)\tLoss 1.3809 (1.3713)\tPrec@1 (49.457)\n",
      "[360/391]\tTime 0.280 (0.277)\tData 0.020 (0.020)\tLoss 1.1864 (1.3581)\tPrec@1 (50.048)\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.035 (0.035)\tLoss 1.2344 (1.2344)\tPrec@1 53.906 (53.906)\n",
      "Test: [40/79]\tTime 0.035 (0.034)\tLoss 1.3750 (1.2795)\tPrec@1 54.688 (55.469)\n",
      " * ResNet110 Prec@1 55.140\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.021 (0.021)\tLoss 1.2312 (1.2312)\tPrec@1 56.250 (56.250)\n",
      "Test: [40/79]\tTime 0.021 (0.021)\tLoss 1.4202 (1.4624)\tPrec@1 46.094 (52.934)\n",
      " * ResNet32 Prec@1 53.290\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.019 (0.019)\tLoss 1.3477 (1.3477)\tPrec@1 53.906 (53.906)\n",
      "Test: [40/79]\tTime 0.018 (0.019)\tLoss 1.4841 (1.5506)\tPrec@1 47.656 (49.543)\n",
      " * ResNet16 Prec@1 49.490\n",
      "\n",
      "===> epoch: 3/200\n",
      "Training:\n",
      "[0/391]\tTime 0.291 (0.291)\tData 0.021 (0.021)\tLoss 1.1430 (1.1430)\tPrec@1 (59.375)\n",
      "[40/391]\tTime 0.278 (0.281)\tData 0.020 (0.020)\tLoss 1.1620 (1.1958)\tPrec@1 (56.441)\n",
      "[80/391]\tTime 0.278 (0.281)\tData 0.019 (0.020)\tLoss 1.1266 (1.1715)\tPrec@1 (57.215)\n",
      "[120/391]\tTime 0.280 (0.281)\tData 0.019 (0.020)\tLoss 0.9364 (1.1713)\tPrec@1 (57.393)\n",
      "[160/391]\tTime 0.282 (0.282)\tData 0.020 (0.020)\tLoss 1.0533 (1.1604)\tPrec@1 (58.007)\n",
      "[200/391]\tTime 0.287 (0.282)\tData 0.019 (0.020)\tLoss 0.9997 (1.1439)\tPrec@1 (58.431)\n",
      "[240/391]\tTime 0.287 (0.283)\tData 0.019 (0.020)\tLoss 1.2093 (1.1344)\tPrec@1 (58.801)\n",
      "[280/391]\tTime 0.280 (0.283)\tData 0.019 (0.020)\tLoss 0.9959 (1.1222)\tPrec@1 (59.308)\n",
      "[320/391]\tTime 0.284 (0.284)\tData 0.019 (0.020)\tLoss 1.0202 (1.1107)\tPrec@1 (59.808)\n",
      "[360/391]\tTime 0.310 (0.284)\tData 0.019 (0.020)\tLoss 0.9270 (1.0978)\tPrec@1 (60.316)\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.036 (0.036)\tLoss 1.0817 (1.0817)\tPrec@1 60.156 (60.156)\n",
      "Test: [40/79]\tTime 0.036 (0.035)\tLoss 1.2247 (1.1249)\tPrec@1 65.625 (62.100)\n",
      " * ResNet110 Prec@1 61.950\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.022 (0.022)\tLoss 1.3479 (1.3479)\tPrec@1 61.719 (61.719)\n",
      "Test: [40/79]\tTime 0.021 (0.021)\tLoss 1.3720 (1.4135)\tPrec@1 57.031 (60.652)\n",
      " * ResNet32 Prec@1 60.810\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.019 (0.019)\tLoss 1.1807 (1.1807)\tPrec@1 59.375 (59.375)\n",
      "Test: [40/79]\tTime 0.019 (0.019)\tLoss 1.1370 (1.1511)\tPrec@1 56.250 (60.766)\n",
      " * ResNet16 Prec@1 60.340\n",
      "\n",
      "===> epoch: 4/200\n",
      "Training:\n",
      "[0/391]\tTime 0.311 (0.311)\tData 0.021 (0.021)\tLoss 1.1426 (1.1426)\tPrec@1 (59.375)\n",
      "[40/391]\tTime 0.281 (0.294)\tData 0.020 (0.020)\tLoss 0.9377 (0.9923)\tPrec@1 (64.101)\n",
      "[80/391]\tTime 0.294 (0.297)\tData 0.020 (0.020)\tLoss 0.9623 (0.9863)\tPrec@1 (64.680)\n",
      "[120/391]\tTime 0.286 (0.294)\tData 0.019 (0.020)\tLoss 0.8701 (0.9816)\tPrec@1 (64.837)\n",
      "[160/391]\tTime 0.294 (0.294)\tData 0.020 (0.020)\tLoss 0.9748 (0.9680)\tPrec@1 (65.411)\n",
      "[200/391]\tTime 0.294 (0.294)\tData 0.020 (0.020)\tLoss 0.9066 (0.9599)\tPrec@1 (65.757)\n",
      "[240/391]\tTime 0.297 (0.295)\tData 0.019 (0.020)\tLoss 1.2102 (0.9503)\tPrec@1 (66.053)\n",
      "[280/391]\tTime 0.299 (0.295)\tData 0.020 (0.020)\tLoss 0.8587 (0.9448)\tPrec@1 (66.273)\n",
      "[320/391]\tTime 0.314 (0.296)\tData 0.019 (0.020)\tLoss 0.8979 (0.9410)\tPrec@1 (66.392)\n",
      "[360/391]\tTime 0.300 (0.296)\tData 0.020 (0.020)\tLoss 0.9100 (0.9370)\tPrec@1 (66.514)\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.036 (0.036)\tLoss 0.9786 (0.9786)\tPrec@1 64.062 (64.062)\n",
      "Test: [40/79]\tTime 0.035 (0.036)\tLoss 0.9861 (0.9743)\tPrec@1 64.062 (66.768)\n",
      " * ResNet110 Prec@1 66.690\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.022 (0.022)\tLoss 1.0528 (1.0528)\tPrec@1 64.062 (64.062)\n",
      "Test: [40/79]\tTime 0.022 (0.022)\tLoss 1.3345 (1.1515)\tPrec@1 58.594 (62.824)\n",
      " * ResNet32 Prec@1 63.020\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.019 (0.019)\tLoss 1.3958 (1.3958)\tPrec@1 60.938 (60.938)\n",
      "Test: [40/79]\tTime 0.019 (0.019)\tLoss 1.3741 (1.3195)\tPrec@1 57.812 (59.642)\n",
      " * ResNet16 Prec@1 58.900\n",
      "\n",
      "===> epoch: 5/200\n",
      "Training:\n",
      "[0/391]\tTime 0.312 (0.312)\tData 0.021 (0.021)\tLoss 0.9230 (0.9230)\tPrec@1 (67.969)\n",
      "[40/391]\tTime 0.302 (0.294)\tData 0.024 (0.019)\tLoss 0.9669 (0.8731)\tPrec@1 (68.655)\n",
      "[80/391]\tTime 0.300 (0.294)\tData 0.019 (0.019)\tLoss 0.7959 (0.8580)\tPrec@1 (69.522)\n",
      "[120/391]\tTime 0.297 (0.296)\tData 0.020 (0.019)\tLoss 0.8289 (0.8589)\tPrec@1 (69.467)\n",
      "[160/391]\tTime 0.299 (0.297)\tData 0.020 (0.019)\tLoss 0.8647 (0.8505)\tPrec@1 (69.856)\n",
      "[200/391]\tTime 0.311 (0.299)\tData 0.019 (0.019)\tLoss 0.7711 (0.8414)\tPrec@1 (70.266)\n",
      "[240/391]\tTime 0.298 (0.300)\tData 0.019 (0.020)\tLoss 0.7376 (0.8380)\tPrec@1 (70.342)\n",
      "[280/391]\tTime 0.438 (0.301)\tData 0.019 (0.020)\tLoss 0.7601 (0.8302)\tPrec@1 (70.588)\n",
      "[320/391]\tTime 0.296 (0.302)\tData 0.020 (0.020)\tLoss 0.8865 (0.8283)\tPrec@1 (70.648)\n",
      "[360/391]\tTime 0.291 (0.302)\tData 0.020 (0.020)\tLoss 0.7135 (0.8263)\tPrec@1 (70.806)\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.038 (0.038)\tLoss 0.9216 (0.9216)\tPrec@1 68.750 (68.750)\n",
      "Test: [40/79]\tTime 0.036 (0.036)\tLoss 1.1458 (1.0575)\tPrec@1 63.281 (65.873)\n",
      " * ResNet110 Prec@1 66.240\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.021 (0.021)\tLoss 0.6873 (0.6873)\tPrec@1 78.906 (78.906)\n",
      "Test: [40/79]\tTime 0.022 (0.022)\tLoss 1.1574 (0.9244)\tPrec@1 62.500 (70.084)\n",
      " * ResNet32 Prec@1 70.250\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.019 (0.019)\tLoss 1.1178 (1.1178)\tPrec@1 64.844 (64.844)\n",
      "Test: [40/79]\tTime 0.019 (0.019)\tLoss 1.4789 (1.2934)\tPrec@1 56.250 (59.985)\n",
      " * ResNet16 Prec@1 60.260\n",
      "\n",
      "===> epoch: 6/200\n",
      "Training:\n",
      "[0/391]\tTime 0.311 (0.311)\tData 0.021 (0.021)\tLoss 0.7100 (0.7100)\tPrec@1 (76.562)\n",
      "[40/391]\tTime 0.311 (0.305)\tData 0.021 (0.020)\tLoss 0.8345 (0.7565)\tPrec@1 (74.028)\n",
      "[80/391]\tTime 0.298 (0.302)\tData 0.019 (0.020)\tLoss 0.7330 (0.7542)\tPrec@1 (73.688)\n",
      "[120/391]\tTime 0.302 (0.302)\tData 0.020 (0.020)\tLoss 0.8402 (0.7624)\tPrec@1 (73.212)\n",
      "[160/391]\tTime 0.285 (0.304)\tData 0.019 (0.020)\tLoss 0.6674 (0.7616)\tPrec@1 (73.146)\n",
      "[200/391]\tTime 0.308 (0.303)\tData 0.019 (0.020)\tLoss 0.7438 (0.7587)\tPrec@1 (73.321)\n",
      "[240/391]\tTime 0.303 (0.304)\tData 0.020 (0.020)\tLoss 0.7399 (0.7582)\tPrec@1 (73.434)\n",
      "[280/391]\tTime 0.309 (0.304)\tData 0.021 (0.020)\tLoss 0.8208 (0.7559)\tPrec@1 (73.438)\n",
      "[320/391]\tTime 0.294 (0.304)\tData 0.020 (0.020)\tLoss 0.8193 (0.7551)\tPrec@1 (73.491)\n",
      "[360/391]\tTime 0.301 (0.304)\tData 0.020 (0.020)\tLoss 0.7642 (0.7501)\tPrec@1 (73.643)\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.037 (0.037)\tLoss 1.0985 (1.0985)\tPrec@1 65.625 (65.625)\n",
      "Test: [40/79]\tTime 0.035 (0.036)\tLoss 1.3938 (1.1832)\tPrec@1 58.594 (63.243)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * ResNet110 Prec@1 63.470\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.022 (0.022)\tLoss 0.7138 (0.7138)\tPrec@1 75.781 (75.781)\n",
      "Test: [40/79]\tTime 0.022 (0.022)\tLoss 0.7712 (0.7154)\tPrec@1 72.656 (75.877)\n",
      " * ResNet32 Prec@1 76.150\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.020 (0.020)\tLoss 0.9502 (0.9502)\tPrec@1 67.969 (67.969)\n",
      "Test: [40/79]\tTime 0.019 (0.019)\tLoss 1.1082 (1.0371)\tPrec@1 62.500 (65.396)\n",
      " * ResNet16 Prec@1 65.110\n",
      "\n",
      "===> epoch: 7/200\n",
      "Training:\n",
      "[0/391]\tTime 0.311 (0.311)\tData 0.021 (0.021)\tLoss 0.7773 (0.7773)\tPrec@1 (71.094)\n",
      "[40/391]\tTime 0.305 (0.303)\tData 0.021 (0.020)\tLoss 0.6742 (0.7162)\tPrec@1 (74.486)\n",
      "[80/391]\tTime 0.297 (0.303)\tData 0.019 (0.020)\tLoss 0.7383 (0.7163)\tPrec@1 (74.701)\n",
      "[120/391]\tTime 0.309 (0.303)\tData 0.020 (0.020)\tLoss 0.5117 (0.7097)\tPrec@1 (74.942)\n",
      "[160/391]\tTime 0.298 (0.303)\tData 0.020 (0.020)\tLoss 0.6782 (0.7077)\tPrec@1 (75.019)\n",
      "[200/391]\tTime 0.312 (0.307)\tData 0.020 (0.020)\tLoss 0.7107 (0.7041)\tPrec@1 (75.124)\n",
      "[240/391]\tTime 0.309 (0.308)\tData 0.020 (0.020)\tLoss 0.6888 (0.7005)\tPrec@1 (75.220)\n",
      "[280/391]\tTime 0.404 (0.308)\tData 0.020 (0.020)\tLoss 0.4993 (0.7007)\tPrec@1 (75.297)\n",
      "[320/391]\tTime 0.294 (0.308)\tData 0.019 (0.020)\tLoss 0.7404 (0.7014)\tPrec@1 (75.246)\n",
      "[360/391]\tTime 0.294 (0.308)\tData 0.019 (0.020)\tLoss 0.6367 (0.6973)\tPrec@1 (75.416)\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.036 (0.036)\tLoss 0.6485 (0.6485)\tPrec@1 75.781 (75.781)\n",
      "Test: [40/79]\tTime 0.035 (0.036)\tLoss 0.9304 (0.7404)\tPrec@1 68.750 (74.200)\n",
      " * ResNet110 Prec@1 73.940\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.022 (0.022)\tLoss 1.1881 (1.1881)\tPrec@1 61.719 (61.719)\n",
      "Test: [40/79]\tTime 0.021 (0.022)\tLoss 1.2714 (1.2763)\tPrec@1 58.594 (62.843)\n",
      " * ResNet32 Prec@1 62.170\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.019 (0.019)\tLoss 0.8442 (0.8442)\tPrec@1 70.312 (70.312)\n",
      "Test: [40/79]\tTime 0.019 (0.019)\tLoss 1.0713 (0.9806)\tPrec@1 64.062 (66.787)\n",
      " * ResNet16 Prec@1 66.430\n",
      "\n",
      "===> epoch: 8/200\n",
      "Training:\n",
      "[0/391]\tTime 0.311 (0.311)\tData 0.020 (0.020)\tLoss 0.5159 (0.5159)\tPrec@1 (80.469)\n",
      "[40/391]\tTime 0.296 (0.302)\tData 0.019 (0.019)\tLoss 0.7768 (0.6525)\tPrec@1 (77.115)\n",
      "[80/391]\tTime 0.310 (0.303)\tData 0.019 (0.019)\tLoss 0.5737 (0.6576)\tPrec@1 (76.755)\n",
      "[120/391]\tTime 0.304 (0.305)\tData 0.020 (0.019)\tLoss 0.7877 (0.6646)\tPrec@1 (76.582)\n",
      "[160/391]\tTime 0.307 (0.306)\tData 0.019 (0.020)\tLoss 0.6067 (0.6633)\tPrec@1 (76.650)\n",
      "[200/391]\tTime 0.305 (0.308)\tData 0.019 (0.020)\tLoss 0.6360 (0.6588)\tPrec@1 (76.823)\n",
      "[240/391]\tTime 0.343 (0.310)\tData 0.019 (0.020)\tLoss 0.5602 (0.6594)\tPrec@1 (76.780)\n",
      "[280/391]\tTime 0.298 (0.310)\tData 0.020 (0.020)\tLoss 0.7839 (0.6605)\tPrec@1 (76.796)\n",
      "[320/391]\tTime 0.308 (0.309)\tData 0.024 (0.020)\tLoss 0.5547 (0.6578)\tPrec@1 (76.932)\n",
      "[360/391]\tTime 0.312 (0.309)\tData 0.024 (0.020)\tLoss 0.6295 (0.6596)\tPrec@1 (76.889)\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.037 (0.037)\tLoss 1.3902 (1.3902)\tPrec@1 60.938 (60.938)\n",
      "Test: [40/79]\tTime 0.037 (0.037)\tLoss 1.5480 (1.4169)\tPrec@1 56.250 (59.546)\n",
      " * ResNet110 Prec@1 59.410\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.022 (0.022)\tLoss 0.8982 (0.8982)\tPrec@1 72.656 (72.656)\n",
      "Test: [40/79]\tTime 0.022 (0.022)\tLoss 1.1740 (0.9903)\tPrec@1 67.188 (69.684)\n",
      " * ResNet32 Prec@1 69.530\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.019 (0.019)\tLoss 1.0533 (1.0533)\tPrec@1 67.969 (67.969)\n",
      "Test: [40/79]\tTime 0.019 (0.019)\tLoss 1.1120 (1.0522)\tPrec@1 64.062 (66.101)\n",
      " * ResNet16 Prec@1 65.630\n",
      "\n",
      "===> epoch: 9/200\n",
      "Training:\n",
      "[0/391]\tTime 0.311 (0.311)\tData 0.021 (0.021)\tLoss 0.6091 (0.6091)\tPrec@1 (79.688)\n",
      "[40/391]\tTime 0.301 (0.303)\tData 0.020 (0.020)\tLoss 0.4808 (0.6111)\tPrec@1 (78.963)\n",
      "[80/391]\tTime 0.310 (0.313)\tData 0.020 (0.020)\tLoss 0.5385 (0.6123)\tPrec@1 (78.877)\n",
      "[120/391]\tTime 0.304 (0.313)\tData 0.020 (0.020)\tLoss 0.4909 (0.6185)\tPrec@1 (78.564)\n",
      "[160/391]\tTime 0.294 (0.313)\tData 0.020 (0.020)\tLoss 0.3945 (0.6261)\tPrec@1 (78.324)\n",
      "[200/391]\tTime 0.312 (0.313)\tData 0.020 (0.020)\tLoss 0.6817 (0.6319)\tPrec@1 (78.137)\n",
      "[240/391]\tTime 0.311 (0.313)\tData 0.020 (0.020)\tLoss 0.7025 (0.6306)\tPrec@1 (78.261)\n",
      "[280/391]\tTime 0.305 (0.312)\tData 0.020 (0.020)\tLoss 0.6703 (0.6282)\tPrec@1 (78.356)\n",
      "[320/391]\tTime 0.304 (0.311)\tData 0.019 (0.020)\tLoss 0.6190 (0.6278)\tPrec@1 (78.327)\n",
      "[360/391]\tTime 0.307 (0.312)\tData 0.020 (0.020)\tLoss 0.6342 (0.6299)\tPrec@1 (78.095)\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.038 (0.038)\tLoss 0.7448 (0.7448)\tPrec@1 68.750 (68.750)\n",
      "Test: [40/79]\tTime 0.037 (0.037)\tLoss 0.9367 (0.7973)\tPrec@1 64.844 (72.542)\n",
      " * ResNet110 Prec@1 72.610\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.022 (0.022)\tLoss 1.0243 (1.0243)\tPrec@1 67.188 (67.188)\n",
      "Test: [40/79]\tTime 0.022 (0.022)\tLoss 1.0329 (0.9332)\tPrec@1 68.750 (70.884)\n",
      " * ResNet32 Prec@1 71.050\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.019 (0.019)\tLoss 0.7538 (0.7538)\tPrec@1 70.312 (70.312)\n",
      "Test: [40/79]\tTime 0.019 (0.019)\tLoss 0.8425 (0.7750)\tPrec@1 69.531 (74.066)\n",
      " * ResNet16 Prec@1 74.070\n",
      "\n",
      "===> epoch: 10/200\n",
      "Training:\n",
      "[0/391]\tTime 0.311 (0.311)\tData 0.021 (0.021)\tLoss 0.6434 (0.6434)\tPrec@1 (78.906)\n",
      "[40/391]\tTime 0.307 (0.309)\tData 0.019 (0.020)\tLoss 0.6783 (0.5922)\tPrec@1 (79.268)\n",
      "[80/391]\tTime 0.296 (0.306)\tData 0.020 (0.020)\tLoss 0.6730 (0.5861)\tPrec@1 (79.292)\n",
      "[120/391]\tTime 0.307 (0.306)\tData 0.019 (0.020)\tLoss 0.7171 (0.5951)\tPrec@1 (78.932)\n",
      "[160/391]\tTime 0.308 (0.306)\tData 0.026 (0.020)\tLoss 0.6816 (0.5985)\tPrec@1 (78.901)\n",
      "[200/391]\tTime 0.312 (0.308)\tData 0.020 (0.020)\tLoss 0.5753 (0.6034)\tPrec@1 (78.762)\n",
      "[240/391]\tTime 0.438 (0.309)\tData 0.019 (0.020)\tLoss 0.7850 (0.6027)\tPrec@1 (78.858)\n",
      "[280/391]\tTime 0.310 (0.309)\tData 0.020 (0.020)\tLoss 0.8115 (0.6027)\tPrec@1 (78.903)\n",
      "[320/391]\tTime 0.308 (0.309)\tData 0.019 (0.020)\tLoss 0.6787 (0.6005)\tPrec@1 (78.974)\n",
      "[360/391]\tTime 0.532 (0.309)\tData 0.020 (0.020)\tLoss 0.5028 (0.6016)\tPrec@1 (78.908)\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.037 (0.037)\tLoss 0.6918 (0.6918)\tPrec@1 76.562 (76.562)\n",
      "Test: [40/79]\tTime 0.036 (0.036)\tLoss 1.0043 (0.9108)\tPrec@1 73.438 (71.341)\n",
      " * ResNet110 Prec@1 71.270\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.025 (0.025)\tLoss 0.5988 (0.5988)\tPrec@1 80.469 (80.469)\n",
      "Test: [40/79]\tTime 0.022 (0.025)\tLoss 0.7351 (0.6566)\tPrec@1 77.344 (78.373)\n",
      " * ResNet32 Prec@1 78.330\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.019 (0.019)\tLoss 0.6267 (0.6267)\tPrec@1 77.344 (77.344)\n",
      "Test: [40/79]\tTime 0.019 (0.019)\tLoss 0.9820 (0.7445)\tPrec@1 72.656 (75.476)\n",
      " * ResNet16 Prec@1 75.300\n",
      "\n",
      "===> epoch: 11/200\n",
      "Training:\n",
      "[0/391]\tTime 0.311 (0.311)\tData 0.021 (0.021)\tLoss 0.5332 (0.5332)\tPrec@1 (80.469)\n",
      "[40/391]\tTime 0.309 (0.310)\tData 0.019 (0.020)\tLoss 0.5052 (0.5641)\tPrec@1 (79.707)\n",
      "[80/391]\tTime 0.306 (0.308)\tData 0.020 (0.020)\tLoss 0.6842 (0.5767)\tPrec@1 (79.417)\n",
      "[120/391]\tTime 0.346 (0.309)\tData 0.020 (0.020)\tLoss 0.4358 (0.5831)\tPrec@1 (79.474)\n",
      "[160/391]\tTime 0.296 (0.309)\tData 0.020 (0.020)\tLoss 0.5100 (0.5823)\tPrec@1 (79.581)\n",
      "[200/391]\tTime 0.309 (0.309)\tData 0.020 (0.020)\tLoss 0.6109 (0.5784)\tPrec@1 (79.707)\n",
      "[240/391]\tTime 0.319 (0.309)\tData 0.020 (0.020)\tLoss 0.6429 (0.5778)\tPrec@1 (79.668)\n",
      "[280/391]\tTime 0.343 (0.310)\tData 0.020 (0.020)\tLoss 0.5715 (0.5839)\tPrec@1 (79.521)\n",
      "[320/391]\tTime 0.312 (0.312)\tData 0.020 (0.020)\tLoss 0.3971 (0.5819)\tPrec@1 (79.563)\n",
      "[360/391]\tTime 0.304 (0.311)\tData 0.020 (0.020)\tLoss 0.4280 (0.5807)\tPrec@1 (79.571)\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.037 (0.037)\tLoss 0.6425 (0.6425)\tPrec@1 79.688 (79.688)\n",
      "Test: [40/79]\tTime 0.036 (0.036)\tLoss 0.6740 (0.6483)\tPrec@1 73.438 (78.678)\n",
      " * ResNet110 Prec@1 78.510\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.022 (0.022)\tLoss 0.6995 (0.6995)\tPrec@1 78.125 (78.125)\n",
      "Test: [40/79]\tTime 0.022 (0.022)\tLoss 0.6857 (0.7318)\tPrec@1 76.562 (76.181)\n",
      " * ResNet32 Prec@1 75.970\n",
      "Testing:\n",
      "Test: [0/79]\tTime 0.019 (0.019)\tLoss 0.9194 (0.9194)\tPrec@1 69.531 (69.531)\n",
      "Test: [40/79]\tTime 0.019 (0.019)\tLoss 1.0360 (0.9211)\tPrec@1 67.969 (70.751)\n",
      " * ResNet16 Prec@1 70.800\n",
      "\n",
      "===> epoch: 12/200\n",
      "Training:\n",
      "[0/391]\tTime 0.311 (0.311)\tData 0.021 (0.021)\tLoss 0.6079 (0.6079)\tPrec@1 (80.469)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "from visdom import Visdom\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from utils import *\n",
    "\n",
    "# Teacher models:\n",
    "# VGG11/VGG13/VGG16/VGG19, GoogLeNet, AlxNet, ResNet18, ResNet34, \n",
    "# ResNet50, ResNet101, ResNet152, ResNeXt29_2x64d, ResNeXt29_4x64d, \n",
    "# ResNeXt29_8x64d, ResNeXt29_32x64d, PreActResNet18, PreActResNet34, \n",
    "# PreActResNet50, PreActResNet101, PreActResNet152, \n",
    "# DenseNet121, DenseNet161, DenseNet169, DenseNet201, \n",
    "import models\n",
    "\n",
    "# Student models:\n",
    "# myNet, LeNet, FitNet\n",
    "\n",
    "start_time = time.time()\n",
    "# os.makedirs('./checkpoint', exist_ok=True)\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch DML 3S')\n",
    "\n",
    "parser.add_argument('--dataset',\n",
    "                    choices=['CIFAR10',\n",
    "                             'CIFAR100'\n",
    "                            ],\n",
    "                    default='CIFAR10')\n",
    "parser.add_argument('--net1',\n",
    "                    choices=['ResNet8',\n",
    "                             'ResNet15',\n",
    "                             'ResNet16',\n",
    "                             'ResNet20',\n",
    "                             'ResNet32',\n",
    "                             'ResNet50',\n",
    "                             'ResNet56',\n",
    "                             'ResNet110'\n",
    "                            ],\n",
    "                    default='ResNet20')\n",
    "parser.add_argument('--net2',\n",
    "                    choices=['ResNet8',\n",
    "                             'ResNet15',\n",
    "                             'ResNet16',\n",
    "                             'ResNet20',\n",
    "                             'ResNet32',\n",
    "                             'ResNet50',\n",
    "                             'ResNet56',\n",
    "                             'ResNet110'\n",
    "                            ],\n",
    "                    default='ResNet56')\n",
    "\n",
    "parser.add_argument('--net3',\n",
    "                    choices=['ResNet8',\n",
    "                             'ResNet15',\n",
    "                             'ResNet16',\n",
    "                             'ResNet20',\n",
    "                             'ResNet32',\n",
    "                             'ResNet50',\n",
    "                             'ResNet56',\n",
    "                             'ResNet110'\n",
    "                            ],\n",
    "                    default='ResNet110')\n",
    "\n",
    "parser.add_argument('--n_class', type=int, default=10, metavar='N', help='num of classes')\n",
    "parser.add_argument('--batch_size', type=int, default=128, metavar='N', help='input batch size for training')\n",
    "parser.add_argument('--test_batch_size', type=int, default=128, metavar='N', help='input test batch size for training')\n",
    "parser.add_argument('--epochs', type=int, default=20, metavar='N', help='number of epochs to train (default: 20)')\n",
    "parser.add_argument('--lr', type=float, default=0.1, metavar='LR', help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, metavar='M', help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--device', default='cuda:1', type=str, help='device: cuda or cpu')\n",
    "parser.add_argument('--print_freq', type=int, default=40, metavar='N', help='how many batches to wait before logging training status')\n",
    "\n",
    "config = ['--dataset', 'CIFAR10', '--epochs', '200', '--n_class', '100', '--net1', 'ResNet110', '--net2', 'ResNet32', '--net3', 'ResNet16', '--device', 'cuda:1']\n",
    "args = parser.parse_args(config)\n",
    "\n",
    "device = args.device if torch.cuda.is_available() else 'cpu'\n",
    "save_dir = './checkpoint/' + args.dataset + '/'\n",
    "\n",
    "# models\n",
    "net1 = getattr(models, args.net1)(num_classes=args.n_class)\n",
    "net1.to(device)\n",
    "net2 = getattr(models, args.net2)(num_classes=args.n_class)\n",
    "net2.to(device)\n",
    "net3 = getattr(models, args.net3)(num_classes=args.n_class)\n",
    "net3.to(device)\n",
    "\n",
    "# logging\n",
    "logfile = save_dir + 'DML_3S_.log'\n",
    "if os.path.exists(logfile):\n",
    "    os.remove(logfile)\n",
    "def log_out(info):\n",
    "    f = open(logfile, mode='a')\n",
    "    f.write(info)\n",
    "    f.write('\\n')\n",
    "    f.close()\n",
    "    print(info)\n",
    "    \n",
    "# visualizer\n",
    "vis = Visdom(env='distill')\n",
    "loss_win = vis.line(\n",
    "    X=np.column_stack((0,0,0)),\n",
    "    Y=np.column_stack((0,0,0)),\n",
    "    opts=dict(\n",
    "        title='DML 3S Loss',\n",
    "        xlabel='epoch',\n",
    "        xtickmin=0,\n",
    "        ylabel='loss',\n",
    "        ytickmin=0,\n",
    "        ytickstep=0.5,\n",
    "        legend=['net1_loss', 'net2_loss', 'net3_loss']\n",
    "    ),\n",
    "    name=\"loss\"\n",
    ")\n",
    "\n",
    "acc_win = vis.line(\n",
    "    X=np.column_stack((0,0,0)),\n",
    "    Y=np.column_stack((0,0,0)),\n",
    "    opts=dict(\n",
    "        title='DML 3S Acc',\n",
    "        xlabel='epoch',\n",
    "        xtickmin=0,\n",
    "        ylabel='accuracy',\n",
    "        ytickmin=0,\n",
    "        ytickmax=100,\n",
    "        legend=['net1_acc', 'net2_acc', 'net3_acc']\n",
    "    ),\n",
    "    name=\"acc\"\n",
    ")\n",
    "\n",
    "\n",
    "# data\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, 4),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "test_transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "train_set = getattr(datasets, args.dataset)(root='../data', train=True, download=True, transform=train_transform)\n",
    "test_set = getattr(datasets, args.dataset)(root='../data', train=False, download=False, transform=test_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=args.test_batch_size, shuffle=False)\n",
    "\n",
    "# optimizer = optim.SGD(st_model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "optimizer_1 = optim.SGD(net1.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
    "optimizer_2 = optim.SGD(net2.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
    "optimizer_3 = optim.SGD(net3.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
    "lr_scheduler_1 = optim.lr_scheduler.MultiStepLR(optimizer_1, milestones=[100, 150])\n",
    "lr_scheduler_2 = optim.lr_scheduler.MultiStepLR(optimizer_2, milestones=[100, 150])\n",
    "lr_scheduler_3 = optim.lr_scheduler.MultiStepLR(optimizer_3, milestones=[100, 150])\n",
    "\n",
    "\n",
    "# train with multi-teacher\n",
    "def train(epoch, net1, net2, net3):\n",
    "    print('Training:')\n",
    "    # switch to train mode\n",
    "    net1.train()\n",
    "    net2.train()\n",
    "    net3.train()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses_1 = AverageMeter()\n",
    "    losses_2 = AverageMeter()\n",
    "    losses_3 = AverageMeter()\n",
    "    top1_1 = AverageMeter()\n",
    "    top1_2 = AverageMeter()\n",
    "    top1_3 = AverageMeter()\n",
    "    \n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        \n",
    "        # compute outputs\n",
    "        _,_,_,_, output_1 = net1(input)\n",
    "        _,_,_,_, output_2 = net2(input)\n",
    "        _,_,_,_, output_3 = net3(input)\n",
    "        logits_1 = F.softmax(output_1)\n",
    "        logits_2 = F.softmax(output_2)\n",
    "        logits_3 = F.softmax(output_3)\n",
    "        \n",
    "        optimizer_1.zero_grad()\n",
    "        loss_1 = 0.5 * (nn.KLDivLoss()(logits_2, logits_1) + nn.KLDivLoss()(logits_3, logits_1)) + F.cross_entropy(output_1, target)\n",
    "        loss_1.backward(retain_graph=True)\n",
    "        optimizer_1.step()\n",
    "        \n",
    "        optimizer_2.zero_grad()\n",
    "        loss_2 = 0.5 * (nn.KLDivLoss()(logits_1, logits_2) + nn.KLDivLoss()(logits_3, logits_2)) + F.cross_entropy(output_2, target)\n",
    "        loss_2.backward(retain_graph=True)\n",
    "        optimizer_2.step()\n",
    "        \n",
    "        optimizer_3.zero_grad()\n",
    "        loss_3 = 0.5 * (nn.KLDivLoss()(logits_1, logits_3) + nn.KLDivLoss()(logits_2, logits_3)) + F.cross_entropy(output_3, target)\n",
    "        loss_3.backward(retain_graph=True)\n",
    "        optimizer_3.step()\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        net1_acc = accuracy(output_1, target)[0]\n",
    "        net2_acc = accuracy(output_2, target)[0]\n",
    "        net3_acc = accuracy(output_3, target)[0]\n",
    "        losses_1.update(loss_1.item(), input.size(0))\n",
    "        top1_1.update(net1_acc, input.size(0))\n",
    "        losses_2.update(loss_2.item(), input.size(0))\n",
    "        top1_2.update(net2_acc, input.size(0))\n",
    "        losses_3.update(loss_3.item(), input.size(0))\n",
    "        top1_3.update(net3_acc, input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            log_out('[{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 ({top1_1.avg:.3f})'.format(\n",
    "                      i, len(train_loader), batch_time=batch_time,\n",
    "                      data_time=data_time, loss=losses_1, top1_1=top1_1))\n",
    "    return losses_1.avg, losses_2.avg, losses_3.avg, net1_acc.cpu().numpy(), net2_acc.cpu().numpy(), net3_acc.cpu().numpy()\n",
    "\n",
    "\n",
    "def test(model):\n",
    "    print('Testing:')\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(test_loader):\n",
    "            input, target = input.to(device), target.to(device)\n",
    "\n",
    "            # compute output\n",
    "            _,_,_,_,output = model(input)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "\n",
    "            output = output.float()\n",
    "            loss = loss.float()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            test_acc = accuracy(output.data, target.data)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(test_acc, input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % args.print_freq == 0:\n",
    "                log_out('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                          i, len(test_loader), batch_time=batch_time, loss=losses,\n",
    "                          top1=top1))\n",
    "\n",
    "    log_out(' * {0} Prec@1 {top1.avg:.3f}'.format(model.model_name, top1=top1))\n",
    "\n",
    "    return losses.avg, test_acc.cpu().numpy(), top1.avg.cpu().numpy()\n",
    "\n",
    "\n",
    "print('*-----------------DML----------------*')\n",
    "best_acc = 0\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    log_out(\"\\n===> epoch: {}/{}\".format(epoch, args.epochs))\n",
    "#     log_out('current lr {:.5e}'.format(optimizer_1.param_groups[0]['lr']))\n",
    "    lr_scheduler_1.step()\n",
    "    lr_scheduler_2.step()\n",
    "    lr_scheduler_3.step()\n",
    "    train_loss_1, train_loss_2, train_loss_3, net1_acc, net2_acc, net3_acc = train(epoch, net1, net2, net3)\n",
    "    # visaulize loss\n",
    "    vis.line(np.column_stack((train_loss_1, train_loss_2, train_loss_3)), np.column_stack((epoch, epoch, epoch)), loss_win, update=\"append\")\n",
    "    _, test_acc_1, top1_1 = test(net1)\n",
    "    _, test_acc_2, top1_2 = test(net2)\n",
    "    _, test_acc_3, top1_3 = test(net3)\n",
    "    vis.line(np.column_stack((top1_1, top1_2, top1_3)), np.column_stack((epoch, epoch, epoch)), acc_win, update=\"append\")\n",
    "    best_acc = max(top1_1, top1_2, top1_3, best_acc)\n",
    "\n",
    "log_out(\"@ BEST Prec: {:.4f}\".format(best_acc))\n",
    "log_out(\"--- {:.3f} mins ---\".format((time.time() - start_time)/60))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
